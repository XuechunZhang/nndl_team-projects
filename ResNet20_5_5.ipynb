{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Group Project\n",
    "\n",
    "## Topic: Towards Accurate Binary Convolutional Neural Network\n",
    "## Group Member: Qichen Hu, Xuechun Zhang, Yingtong Han\n",
    "\n",
    "In this project, we replicated the results in Towards Accurate Binary Convolutional Neural Network [Lin et al., 2017]. The core idea is to approximate the weights of convolution layer by binarized weights for acceleration purpose. We built the model based on tensorflow 1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO') # disable warnings\n",
    "import numpy as np\n",
    "\n",
    "# import utils functions\n",
    "from utils_functions import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (49000, 32, 32, 3)\n",
      "x_val shape: (1000, 32, 32, 3)\n",
      "y_train shape: (49000, 1)\n",
      "y_val shape: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load cifar10 data\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "# scale data to [0,1]\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# train validation split\n",
    "x_train, x_val = x_train[:-1000], x_train[-1000:]\n",
    "y_train, y_val = y_train[:-1000], y_train[-1000:]\n",
    "\n",
    "# check shape\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('x_val shape:',x_val.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('y_val shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph of Traditional ResNet200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-6e4e57d1b1fc>:28: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-6e4e57d1b1fc>:115: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-6e4e57d1b1fc>:132: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the full-precision ResNet20 graph without ABC\n",
    "num_res_blocks = 3\n",
    "full_precision_graph = tf.Graph()\n",
    "with full_precision_graph.as_default():\n",
    "    # Defining inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Regularization term\n",
    "    regularizer = 0\n",
    "    reg = 0.01\n",
    "    \n",
    "    # Create a variable dict to save weights\n",
    "    full_precision_variables = {}\n",
    "    \n",
    "    # Process inputs into the required form\n",
    "    num_filters_out = 16\n",
    "    \n",
    "    with tf.variable_scope('resnet_block', reuse=False):\n",
    "        # Record layer index\n",
    "        i = 1\n",
    "        j = 1\n",
    "        \n",
    "        # Convolution Layer 1 before the resnet block\n",
    "        W_conv1 = weight_variable(shape=([3,3,3,num_filters_out]), name=\"W_conv1\")  #kernel size=3, input channels=3\n",
    "        b_conv1 = bias_variable(shape=[num_filters_out], name=\"b_conv1\")\n",
    "        conv1 = (conv2d(x_image, W_conv1) + b_conv1)\n",
    "        bn_conv1 = tf.layers.batch_normalization(conv1, axis=-1, name=\"bn_conv1\")\n",
    "        h_conv1 = tf.nn.relu(bn_conv1)\n",
    "        \n",
    "        full_precision_variables['W_conv1'] = W_conv1\n",
    "        full_precision_variables['b_conv1'] = b_conv1\n",
    "        \n",
    "        # Start resnet block\n",
    "        i += 1\n",
    "        \n",
    "        for stack in range(3):\n",
    "            for res_block in range(num_res_blocks):\n",
    "                \n",
    "                strides_in = [1,1,1,1]\n",
    "                num_filters_in = num_filters_out\n",
    "                num_filters_initial_block = num_filters_in\n",
    "                \n",
    "                #--------------block layer: y path - first y\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = [1,2,2,1]  # downsample\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                globals()['W_conv' + str(i)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = bias_variable(shape=[num_filters_out], name='b_conv' + str(i)) \n",
    "                globals()['conv' + str(i)] = (conv2d(globals()['h_conv' + str(i-1)], globals()['W_conv' + str(i)], strides_in) + globals()['b_conv' + str(i)])\n",
    "                globals()['bn_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                globals()['h_conv' + str(i)] = tf.nn.relu(globals()['bn_conv' + str(i)])\n",
    "                full_precision_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                full_precision_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                regularizer += tf.nn.l2_loss(globals()['W_conv' + str(i)])\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                num_filters_in = num_filters_out\n",
    "                \n",
    "                #--------------block layer: y path - second y\n",
    "                globals()['W_conv' + str(i)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = bias_variable(shape=[num_filters_out], name='b_conv' + str(i)) \n",
    "                globals()['conv' + str(i)] = (conv2d(globals()['h_conv' + str(i-1)], globals()['W_conv' + str(i)]) + globals()['b_conv' + str(i)])\n",
    "                globals()['h_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                full_precision_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                full_precision_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                regularizer += tf.nn.l2_loss(globals()['W_conv' + str(i)])\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                #--------------block layer: x path\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = [1,2,2,1]  # downsample\n",
    "                    num_filters_in = num_filters_initial_block\n",
    "                    num_filters_out = num_filters_in * 2 \n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    globals()['W_conv_cut' + str(j)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv_cut' + str(j))\n",
    "                    globals()['b_conv_cut' + str(j)] = bias_variable(shape=[num_filters_out], name='b_conv_cut' + str(j))\n",
    "                    globals()['h_conv_cut' + str(j)] = (conv2d(globals()['h_conv' + str(i-3)], globals()['W_conv_cut' + str(j)], strides_in) + globals()['b_conv_cut' + str(j)])\n",
    "                    full_precision_variables['W_conv_cut' + str(j)] = globals()['W_conv_cut' + str(j)]\n",
    "                    full_precision_variables['b_conv_cut' + str(j)] = globals()['b_conv_cut' + str(j)]\n",
    "                    \n",
    "                    regularizer += tf.nn.l2_loss(globals()['W_conv_cut' + str(j)])\n",
    "                    \n",
    "                else:\n",
    "                    globals()['h_conv_cut' + str(j)] = globals()['h_conv' + str(i-3)]\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                #--------------block: sum and acitvate\n",
    "                h_add = tf.add(globals()['h_conv_cut' + str(j-1)] , globals()['h_conv' + str(i-1)])\n",
    "                h_add = tf.nn.relu(h_add)\n",
    "        \n",
    "        # Average pooling\n",
    "        h_pool = tf.nn.max_pool(h_add, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "        \n",
    "        # Flaten the h_add output\n",
    "        h_add_flat = tf.reshape(h_pool, shape=(-1, 2*2*64))\n",
    "        \n",
    "        # Dense layer1\n",
    "        W_fc1 = weight_variable(shape=[2*2*64, 1024], name=\"W_fc1\")\n",
    "        b_fc1 = bias_variable(shape=[1024], name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_add_flat, W_fc1) + b_fc1)\n",
    "        full_precision_variables['W_fc1'] = W_fc1\n",
    "        full_precision_variables['b_fc1'] = b_fc1\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_fc1)\n",
    "        \n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        W_fc2 = weight_variable(shape=[1024, 10], name=\"W_fc2\")\n",
    "        b_fc2 = bias_variable(shape=[10], name=\"b_fc2\")\n",
    "        full_precision_variables['W_fc2'] = W_fc2\n",
    "        full_precision_variables['b_fc2'] = b_fc2\n",
    "        \n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_fc2)\n",
    "        \n",
    "        # Labels\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        y_ = tf.one_hot(y, 10)\n",
    "        \n",
    "        # Defining optimizer and loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy + reg*regularizer)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Initializer\n",
    "        graph_init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet20\n",
    "#### Save the model and weights for initialization of ABC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 32.1429%  Loss: 1.894644\n",
      "Epoch: 2  Val accuracy: 39.8438%  Loss: 1.700185\n",
      "Epoch: 3  Val accuracy: 41.4062%  Loss: 1.595937\n",
      "Epoch: 4  Val accuracy: 44.7545%  Loss: 1.537022\n",
      "Epoch: 5  Val accuracy: 45.2009%  Loss: 1.508383\n",
      "Epoch: 6  Val accuracy: 48.6607%  Loss: 1.452149\n",
      "Epoch: 7  Val accuracy: 50.8929%  Loss: 1.393552\n",
      "Epoch: 8  Val accuracy: 51.4509%  Loss: 1.367378\n",
      "Epoch: 9  Val accuracy: 52.9018%  Loss: 1.331949\n",
      "Epoch: 10  Val accuracy: 54.6875%  Loss: 1.295394\n",
      "Epoch: 11  Val accuracy: 54.4643%  Loss: 1.272193\n",
      "Epoch: 12  Val accuracy: 54.9107%  Loss: 1.242615\n",
      "Epoch: 13  Val accuracy: 57.0312%  Loss: 1.227411\n",
      "Epoch: 14  Val accuracy: 57.4777%  Loss: 1.200356\n",
      "Epoch: 15  Val accuracy: 57.7009%  Loss: 1.176682\n",
      "Epoch: 16  Val accuracy: 60.1562%  Loss: 1.146147\n",
      "Epoch: 17  Val accuracy: 60.8259%  Loss: 1.130648\n",
      "Epoch: 18  Val accuracy: 60.4911%  Loss: 1.120877\n",
      "Epoch: 19  Val accuracy: 61.2723%  Loss: 1.094121\n",
      "Epoch: 20  Val accuracy: 61.7188%  Loss: 1.095253\n",
      "Epoch: 21  Val accuracy: 61.8304%  Loss: 1.087464\n",
      "Epoch: 22  Val accuracy: 61.9420%  Loss: 1.064042\n",
      "Epoch: 23  Val accuracy: 62.0536%  Loss: 1.066011\n",
      "Epoch: 24  Val accuracy: 61.4955%  Loss: 1.068383\n",
      "Epoch: 25  Val accuracy: 62.7232%  Loss: 1.052645\n",
      "Epoch: 26  Val accuracy: 63.9509%  Loss: 1.043552\n",
      "Epoch: 27  Val accuracy: 64.7321%  Loss: 1.024423\n",
      "Epoch: 28  Val accuracy: 65.1786%  Loss: 1.024587\n",
      "Epoch: 29  Val accuracy: 65.2902%  Loss: 1.009031\n",
      "Epoch: 30  Val accuracy: 65.6250%  Loss: 1.011555\n",
      "Epoch: 31  Val accuracy: 65.4018%  Loss: 1.017352\n",
      "Epoch: 32  Val accuracy: 65.2902%  Loss: 1.017048\n",
      "Epoch: 33  Val accuracy: 65.5134%  Loss: 1.007135\n",
      "Epoch: 34  Val accuracy: 66.2946%  Loss: 1.006951\n",
      "Epoch: 35  Val accuracy: 66.2946%  Loss: 1.021870\n",
      "Epoch: 36  Val accuracy: 66.9643%  Loss: 1.014277\n",
      "Epoch: 37  Val accuracy: 65.4018%  Loss: 1.023419\n",
      "Epoch: 38  Val accuracy: 65.8482%  Loss: 1.028438\n",
      "Epoch: 39  Val accuracy: 65.1786%  Loss: 1.066245\n",
      "Epoch: 40  Val accuracy: 62.7232%  Loss: 1.118146\n",
      "CPU times: user 6min 3s, sys: 1min 11s, total: 7min 15s\n",
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "num_epochs = 40\n",
    "batch_size = 128\n",
    "num_batch = len(x_train) // batch_size\n",
    "num_batch_val = len(x_val) // batch_size\n",
    "\n",
    "full_precision_values = {}  # values fed to the ABC model\n",
    "cur_model_name = 'ResNet20_full'\n",
    "pre_trained_model = None\n",
    "\n",
    "with tf.Session(graph=full_precision_graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    if pre_trained_model is not None:\n",
    "        try:\n",
    "            print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "            saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Load model Failed!\")\n",
    "    \n",
    "    sess.run(graph_init)\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(1, num_batch + 1):\n",
    "            X_batch, y_batch = x_train[(iteration-1)*batch_size:iteration*batch_size], y_train[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "            _, acc_train, loss_train = sess.run([train_step, accuracy, cross_entropy],\n",
    "                                                feed_dict={x: X_batch, y: y_batch, keep_prob: 0.8})\n",
    "            \n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%) Train Acc: {:.4f}%\".format(iteration,\n",
    "                                                                           num_batch,\n",
    "                                                                           iteration*100/num_batch,\n",
    "                                                                           acc_train*100),\n",
    "                  end=\"\")\n",
    "        \n",
    "        # At the end of each epoch, measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, num_batch_val + 1):\n",
    "            X_batch, y_batch = x_val[(iteration-1)*batch_size:iteration*batch_size], y_val[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            acc_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, num_batch_val,iteration * 100/num_batch_val),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(epoch + 1, acc_val * 100, loss_val))\n",
    "    \n",
    "    # Save model to file\n",
    "    saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "    \n",
    "    # On completion of training, save the variables to be fed to ABC model\n",
    "    for var_name in full_precision_variables:\n",
    "        full_precision_values[var_name] = sess.run(full_precision_variables[var_name])\n",
    "    \n",
    "    # Save weights to file\n",
    "    f = open(\"Weights/\"+cur_model_name+\".pkl\", \"wb\")\n",
    "    pickle.dump(full_precision_values, f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph of ResNet20 with ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Creating the ResNet20 graph with ABC\n",
    "num_res_blocks = 3\n",
    "ABC_graph = tf.Graph()\n",
    "with ABC_graph.as_default():\n",
    "    # Defining inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Store alpha training and alphas\n",
    "    alpha_training_list = []\n",
    "    alphas_list = []\n",
    "    \n",
    "    # Set ABC hyperparameters\n",
    "    M = 5\n",
    "    N = 5\n",
    "    shift_para = tf.Variable(tf.constant(0., shape=(N,1)), dtype=tf.float32, name=\"shift_para\")\n",
    "    betas = tf.Variable(tf.constant(1., shape=(N,1)), dtype=tf.float32, name=\"betas\")\n",
    "    \n",
    "    # Create a variable dict to save weights\n",
    "    ABC_variables = {}\n",
    "    \n",
    "    # Process inputs into the required form\n",
    "    num_filters_out = 16\n",
    "    \n",
    "    with tf.variable_scope('resnet_block', reuse=False):\n",
    "        # Record layer index\n",
    "        i = 1\n",
    "        j = 1\n",
    "        \n",
    "        # Convolution Layer 1 before the resnet block\n",
    "        W_conv1 = tf.Variable(full_precision_values[\"W_conv1\"], name=\"W_conv1\")\n",
    "        b_conv1 = tf.Variable(full_precision_values[\"b_conv1\"], name=\"b_conv1\")\n",
    "        # ABC\n",
    "        alphas1 = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name=\"alphas1\")\n",
    "        binary_weights1 = calculate_binary_weights(W_conv1, M)\n",
    "        alpha_loss1, alpha_training1 = calculate_alphas(W_conv1, binary_weights1, alphas1, M)\n",
    "        h_conv1 = ABC_layer(x_image, binary_weights1, alphas1, shift_para, betas, M, N, b_conv1)\n",
    "        # Save alpha training and alphas\n",
    "        alpha_training_list.append(alpha_training1)\n",
    "        alphas_list.append(alphas1)\n",
    "        \n",
    "        bn_conv1 = tf.layers.batch_normalization(h_conv1, axis=-1, name=\"bn_conv1\")\n",
    "        h_conv1 = tf.nn.relu(bn_conv1)\n",
    "        \n",
    "        ABC_variables['W_conv1'] = W_conv1\n",
    "        ABC_variables['b_conv1'] = b_conv1\n",
    "        \n",
    "        # Start resnet block\n",
    "        i += 1\n",
    "        \n",
    "        for stack in range(3):\n",
    "            for res_block in range(num_res_blocks):\n",
    "                \n",
    "                strides_in = 1\n",
    "                num_filters_in = num_filters_out\n",
    "                num_filters_initial_block = num_filters_in\n",
    "                \n",
    "                #--------------block layer: y path - first y\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = 2  # downsample\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                globals()['W_conv' + str(i)] = tf.Variable(full_precision_values['W_conv' + str(i)], name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = tf.Variable(full_precision_values['b_conv' + str(i)], name='b_conv' + str(i))\n",
    "                # ABC\n",
    "                globals()['alphas' + str(i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(i))\n",
    "                globals()['binary_weights' + str(i)] = calculate_binary_weights(globals()['W_conv' + str(i)], M)\n",
    "                globals()['alpha_loss' + str(i)], globals()['alpha_training' + str(i)] = calculate_alphas(globals()['W_conv' + str(i)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], M)\n",
    "                globals()['conv' + str(i)] = ABC_layer(globals()['h_conv' + str(i-1)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], shift_para, betas, M, N, globals()['b_conv' + str(i)], strides_in)\n",
    "                # save alpha training and alphas\n",
    "                alpha_training_list.append(globals()['alpha_training' + str(i)])\n",
    "                alphas_list.append(globals()['alphas' + str(i)])\n",
    "                \n",
    "                globals()['bn_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                globals()['h_conv' + str(i)] = tf.nn.relu(globals()['bn_conv' + str(i)])\n",
    "                \n",
    "                ABC_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                ABC_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                num_filters_in = num_filters_out\n",
    "                \n",
    "                #--------------block layer: y path - second y\n",
    "                globals()['W_conv' + str(i)] = tf.Variable(full_precision_values['W_conv' + str(i)], name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = tf.Variable(full_precision_values['b_conv' + str(i)], name='b_conv' + str(i))\n",
    "                # ABC\n",
    "                globals()['alphas' + str(i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(i))\n",
    "                globals()['binary_weights' + str(i)] = calculate_binary_weights(globals()['W_conv' + str(i)], M)\n",
    "                globals()['alpha_loss' + str(i)], globals()['alpha_training' + str(i)] = calculate_alphas(globals()['W_conv' + str(i)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], M)\n",
    "                globals()['conv' + str(i)] = ABC_layer(globals()['h_conv' + str(i-1)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], shift_para, betas, M, N, globals()['b_conv' + str(i)])\n",
    "                # save alpha training and alphas\n",
    "                alpha_training_list.append(globals()['alpha_training' + str(i)])\n",
    "                alphas_list.append(globals()['alphas' + str(i)])\n",
    "                \n",
    "                globals()['h_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                \n",
    "                ABC_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                ABC_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                #--------------block layer: x path\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = 2  # downsample\n",
    "                    num_filters_in = num_filters_initial_block\n",
    "                    num_filters_out = num_filters_in * 2 \n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    globals()['W_conv_cut' + str(j)] = tf.Variable(full_precision_values['W_conv_cut' + str(j)], name='W_conv_cut' + str(j))\n",
    "                    globals()['b_conv_cut' + str(j)] = tf.Variable(full_precision_values['b_conv_cut' + str(j)], name='b_conv_cut' + str(j))\n",
    "                    # ABC\n",
    "                    globals()['alphas' + str(j*100+i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(j*100+i))\n",
    "                    globals()['binary_weights' + str(j*100+i)] = calculate_binary_weights(globals()['W_conv_cut' + str(j)], M)\n",
    "                    globals()['alpha_loss' + str(j*100+i)], globals()['alpha_training' + str(j*100+i)] = calculate_alphas(globals()['W_conv_cut' + str(j)], globals()['binary_weights' + str(j*100+i)], globals()['alphas' + str(j*100+i)], M)\n",
    "                    globals()['h_conv_cut' + str(j)] = ABC_layer(globals()['h_conv' + str(i-3)], globals()['binary_weights' + str(j*100+i)], globals()['alphas' + str(j*100+i)], shift_para, betas, M, N, globals()['b_conv_cut' + str(j)], strides_in)\n",
    "                    # save alpha training and alphas\n",
    "                    alpha_training_list.append(globals()['alpha_training' + str(j*100+i)])\n",
    "                    alphas_list.append(globals()['alphas' + str(j*100+i)])\n",
    "                    \n",
    "                    ABC_variables['W_conv_cut' + str(j)] = globals()['W_conv_cut' + str(j)]\n",
    "                    ABC_variables['b_conv_cut' + str(j)] = globals()['b_conv_cut' + str(j)]\n",
    "                    \n",
    "                else:\n",
    "                    globals()['h_conv_cut' + str(j)] = globals()['h_conv' + str(i-3)]\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                #--------------block: sum and acitvate\n",
    "                h_add = tf.add(globals()['h_conv_cut' + str(j-1)] , globals()['h_conv' + str(i-1)])\n",
    "                h_add = tf.nn.relu(h_add)\n",
    "        \n",
    "        # Average pooling\n",
    "        h_pool = tf.nn.max_pool(h_add, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "        \n",
    "        # Flaten the h_add output\n",
    "        h_add_flat = tf.reshape(h_pool, shape=(-1, 2*2*64))\n",
    "        \n",
    "        # Dense layer1\n",
    "        W_fc1 = tf.Variable(full_precision_values[\"W_fc1\"], name=\"W_fc1\")\n",
    "        b_fc1 = tf.Variable(full_precision_values[\"b_fc1\"], name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_add_flat, W_fc1) + b_fc1)\n",
    "        ABC_variables['W_fc1'] = W_fc1\n",
    "        ABC_variables['b_fc1'] = b_fc1\n",
    "        \n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        W_fc2 = tf.Variable(full_precision_values[\"W_fc2\"], name=\"W_fc2\")\n",
    "        b_fc2 = tf.Variable(full_precision_values[\"b_fc2\"], name=\"b_fc2\")\n",
    "        ABC_variables['W_fc2'] = W_fc2\n",
    "        ABC_variables['b_fc2'] = b_fc2\n",
    "        \n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        # Labels\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        y_ = tf.one_hot(y, 10)\n",
    "        \n",
    "        # Defining optimizer and loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Initializer\n",
    "        graph_init = tf.global_variables_initializer()\n",
    "        alphas_init = tf.variables_initializer(alphas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet20 with ABC\n",
    "#### Save the model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 10.8259%  Loss: 2.263565\n",
      "Epoch: 2  Val accuracy: 16.9643%  Loss: 2.232763\n",
      "Epoch: 3  Val accuracy: 15.2902%  Loss: 2.260489\n",
      "Epoch: 4  Val accuracy: 13.7277%  Loss: 2.255440\n",
      "Epoch: 5  Val accuracy: 18.6384%  Loss: 2.208265\n",
      "Epoch: 6  Val accuracy: 17.6339%  Loss: 2.225171\n",
      "Epoch: 7  Val accuracy: 16.8527%  Loss: 2.198705\n",
      "Epoch: 8  Val accuracy: 19.0848%  Loss: 2.229010\n",
      "Epoch: 9  Val accuracy: 15.2902%  Loss: 2.258009\n",
      "Epoch: 10  Val accuracy: 14.7321%  Loss: 2.698301\n",
      "Epoch: 11  Val accuracy: 17.8571%  Loss: 2.210785\n",
      "Epoch: 12  Val accuracy: 18.5268%  Loss: 2.202560\n",
      "Epoch: 13  Val accuracy: 13.3929%  Loss: 2.274536\n",
      "Epoch: 14  Val accuracy: 15.1786%  Loss: 2.214829\n",
      "Epoch: 15  Val accuracy: 17.7455%  Loss: 2.182756\n",
      "Epoch: 16  Val accuracy: 13.5045%  Loss: 2.250977\n",
      "Epoch: 17  Val accuracy: 16.5179%  Loss: 2.184991\n",
      "Epoch: 18  Val accuracy: 18.7500%  Loss: 2.183599\n",
      "Epoch: 19  Val accuracy: 17.8571%  Loss: 2.207690\n",
      "Epoch: 20  Val accuracy: 19.1964%  Loss: 2.191799\n",
      "CPU times: user 4h 4min 53s, sys: 38min 29s, total: 4h 43min 23s\n",
      "Wall time: 3h 43min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "num_batch = len(x_train) // batch_size\n",
    "num_batch_val = len(x_val) // batch_size\n",
    "num_alpha_epochs = 200\n",
    "\n",
    "ABC_values = {}  # values fed to the ABC model\n",
    "cur_model_name = 'ResNet20_ABC_5_5'\n",
    "pre_trained_model = None\n",
    "\n",
    "with tf.Session(graph=ABC_graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    if pre_trained_model is not None:\n",
    "        try:\n",
    "            print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "            saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Load model Failed!\")\n",
    "    \n",
    "    sess.run(graph_init)\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(1, num_batch + 1):\n",
    "            X_batch, y_batch = x_train[(iteration-1)*batch_size:iteration*batch_size], y_train[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            \n",
    "            # Training alphas\n",
    "            sess.run(alphas_init)\n",
    "            for alpha_train in alpha_training_list:\n",
    "                for alpha_epoch in range(num_alpha_epochs):\n",
    "                    sess.run(alpha_train)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "            _, acc_train, loss_train = sess.run([train_step, accuracy, cross_entropy],\n",
    "                                                feed_dict={x: X_batch, y: y_batch, keep_prob: 0.8})\n",
    "            \n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%) Train Acc: {:.4f}%\".format(iteration,\n",
    "                                                                           num_batch,\n",
    "                                                                           iteration*100/num_batch,\n",
    "                                                                           acc_train*100),\n",
    "                  end=\"\")\n",
    "        \n",
    "        # At the end of each epoch, measure the validation loss and accuracy:\n",
    "        # Training alphas\n",
    "        sess.run(alphas_init)\n",
    "        for alpha_train in alpha_training_list:\n",
    "            for alpha_epoch in range(num_alpha_epochs):\n",
    "                sess.run(alpha_train)\n",
    "        \n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, num_batch_val + 1):\n",
    "            X_batch, y_batch = x_val[(iteration-1)*batch_size:iteration*batch_size], y_val[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            acc_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, num_batch_val,iteration * 100/num_batch_val),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(epoch + 1, acc_val * 100, loss_val))\n",
    "    \n",
    "    # Save model to file\n",
    "    saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "    \n",
    "    # On completion of training, save the variables to be fed to ABC model\n",
    "    for var_name in ABC_variables:\n",
    "        ABC_values[var_name] = sess.run(ABC_variables[var_name])\n",
    "    \n",
    "    # Save weights to file\n",
    "    f = open(\"Weights/\"+cur_model_name+\".pkl\", \"wb\")\n",
    "    pickle.dump(ABC_values, f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
