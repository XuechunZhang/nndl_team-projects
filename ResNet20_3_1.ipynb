{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Group Project\n",
    "\n",
    "## Topic: Towards Accurate Binary Convolutional Neural Network\n",
    "## Group Member: Qichen Hu, Xuechun Zhang, Yingtong Han\n",
    "\n",
    "In this project, we replicated the results in Towards Accurate Binary Convolutional Neural Network [Lin et al., 2017]. The core idea is to approximate the weights of convolution layer by binarized weights for acceleration purpose. We built the model based on tensorflow 1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO') # disable warnings\n",
    "import numpy as np\n",
    "\n",
    "# import utils functions\n",
    "from utils_functions import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (49000, 32, 32, 3)\n",
      "x_val shape: (1000, 32, 32, 3)\n",
      "y_train shape: (49000, 1)\n",
      "y_val shape: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load cifar10 data\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "# scale data to [0,1]\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# train validation split\n",
    "x_train, x_val = x_train[:-1000], x_train[-1000:]\n",
    "y_train, y_val = y_train[:-1000], y_train[-1000:]\n",
    "\n",
    "# check shape\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('x_val shape:',x_val.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('y_val shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph of Traditional ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the full-precision ResNet20 graph without ABC\n",
    "num_res_blocks = 3\n",
    "full_precision_graph = tf.Graph()\n",
    "with full_precision_graph.as_default():\n",
    "    # Defining inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Regularization term\n",
    "    regularizer = 0\n",
    "    reg = 0.01\n",
    "    \n",
    "    # Create a variable dict to save weights\n",
    "    full_precision_variables = {}\n",
    "    \n",
    "    # Process inputs into the required form\n",
    "    num_filters_out = 16\n",
    "    \n",
    "    with tf.variable_scope('resnet_block', reuse=False):\n",
    "        # Record layer index\n",
    "        i = 1\n",
    "        j = 1\n",
    "        \n",
    "        # Convolution Layer 1 before the resnet block\n",
    "        W_conv1 = weight_variable(shape=([3,3,3,num_filters_out]), name=\"W_conv1\")  #kernel size=3, input channels=3\n",
    "        b_conv1 = bias_variable(shape=[num_filters_out], name=\"b_conv1\")\n",
    "        conv1 = (conv2d(x_image, W_conv1) + b_conv1)\n",
    "        bn_conv1 = tf.layers.batch_normalization(conv1, axis=-1, name=\"bn_conv1\")\n",
    "        h_conv1 = tf.nn.relu(bn_conv1)\n",
    "        \n",
    "        full_precision_variables['W_conv1'] = W_conv1\n",
    "        full_precision_variables['b_conv1'] = b_conv1\n",
    "        \n",
    "        # Start resnet block\n",
    "        i += 1\n",
    "        \n",
    "        for stack in range(3):\n",
    "            for res_block in range(num_res_blocks):\n",
    "                \n",
    "                strides_in = [1,1,1,1]\n",
    "                num_filters_in = num_filters_out\n",
    "                num_filters_initial_block = num_filters_in\n",
    "                \n",
    "                #--------------block layer: y path - first y\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = [1,2,2,1]  # downsample\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                globals()['W_conv' + str(i)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = bias_variable(shape=[num_filters_out], name='b_conv' + str(i)) \n",
    "                globals()['conv' + str(i)] = (conv2d(globals()['h_conv' + str(i-1)], globals()['W_conv' + str(i)], strides_in) + globals()['b_conv' + str(i)])\n",
    "                globals()['bn_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                globals()['h_conv' + str(i)] = tf.nn.relu(globals()['bn_conv' + str(i)])\n",
    "                full_precision_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                full_precision_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                regularizer += tf.nn.l2_loss(globals()['W_conv' + str(i)])\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                num_filters_in = num_filters_out\n",
    "                \n",
    "                #--------------block layer: y path - second y\n",
    "                globals()['W_conv' + str(i)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = bias_variable(shape=[num_filters_out], name='b_conv' + str(i)) \n",
    "                globals()['conv' + str(i)] = (conv2d(globals()['h_conv' + str(i-1)], globals()['W_conv' + str(i)]) + globals()['b_conv' + str(i)])\n",
    "                globals()['h_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                full_precision_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                full_precision_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                regularizer += tf.nn.l2_loss(globals()['W_conv' + str(i)])\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                #--------------block layer: x path\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = [1,2,2,1]  # downsample\n",
    "                    num_filters_in = num_filters_initial_block\n",
    "                    num_filters_out = num_filters_in * 2 \n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    globals()['W_conv_cut' + str(j)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv_cut' + str(j))\n",
    "                    globals()['b_conv_cut' + str(j)] = bias_variable(shape=[num_filters_out], name='b_conv_cut' + str(j))\n",
    "                    globals()['h_conv_cut' + str(j)] = (conv2d(globals()['h_conv' + str(i-3)], globals()['W_conv_cut' + str(j)], strides_in) + globals()['b_conv_cut' + str(j)])\n",
    "                    full_precision_variables['W_conv_cut' + str(j)] = globals()['W_conv_cut' + str(j)]\n",
    "                    full_precision_variables['b_conv_cut' + str(j)] = globals()['b_conv_cut' + str(j)]\n",
    "                    \n",
    "                    regularizer += tf.nn.l2_loss(globals()['W_conv_cut' + str(j)])\n",
    "                    \n",
    "                else:\n",
    "                    globals()['h_conv_cut' + str(j)] = globals()['h_conv' + str(i-3)]\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                #--------------block: sum and acitvate\n",
    "                h_add = tf.add(globals()['h_conv_cut' + str(j-1)] , globals()['h_conv' + str(i-1)])\n",
    "                h_add = tf.nn.relu(h_add)\n",
    "        \n",
    "        # Average pooling\n",
    "        h_pool = tf.nn.max_pool(h_add, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "        \n",
    "        # Flaten the h_add output\n",
    "        h_add_flat = tf.reshape(h_pool, shape=(-1, 2*2*64))\n",
    "        \n",
    "        # Dense layer1\n",
    "        W_fc1 = weight_variable(shape=[2*2*64, 1024], name=\"W_fc1\")\n",
    "        b_fc1 = bias_variable(shape=[1024], name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_add_flat, W_fc1) + b_fc1)\n",
    "        full_precision_variables['W_fc1'] = W_fc1\n",
    "        full_precision_variables['b_fc1'] = b_fc1\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_fc1)\n",
    "        \n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        W_fc2 = weight_variable(shape=[1024, 10], name=\"W_fc2\")\n",
    "        b_fc2 = bias_variable(shape=[10], name=\"b_fc2\")\n",
    "        full_precision_variables['W_fc2'] = W_fc2\n",
    "        full_precision_variables['b_fc2'] = b_fc2\n",
    "        \n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_fc2)\n",
    "        \n",
    "        # Labels\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        y_ = tf.one_hot(y, 10)\n",
    "        \n",
    "        # Defining optimizer and loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy + reg*regularizer)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Initializer\n",
    "        graph_init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet20\n",
    "#### Save the model and weights for initialization of ABC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 33.4821%  Loss: 1.867445\n",
      "Epoch: 2  Val accuracy: 38.7277%  Loss: 1.696597\n",
      "Epoch: 3  Val accuracy: 41.6295%  Loss: 1.602691\n",
      "Epoch: 4  Val accuracy: 43.3036%  Loss: 1.513277\n",
      "Epoch: 5  Val accuracy: 47.4330%  Loss: 1.460705\n",
      "Epoch: 6  Val accuracy: 46.0938%  Loss: 1.428325\n",
      "Epoch: 7  Val accuracy: 46.9866%  Loss: 1.401813\n",
      "Epoch: 8  Val accuracy: 50.2232%  Loss: 1.370197\n",
      "Epoch: 9  Val accuracy: 51.5625%  Loss: 1.322718\n",
      "Epoch: 10  Val accuracy: 52.7902%  Loss: 1.298612\n",
      "Epoch: 11  Val accuracy: 52.9018%  Loss: 1.273303\n",
      "Epoch: 12  Val accuracy: 53.9062%  Loss: 1.244815\n",
      "Epoch: 13  Val accuracy: 55.1339%  Loss: 1.209323\n",
      "Epoch: 14  Val accuracy: 57.2545%  Loss: 1.192198\n",
      "Epoch: 15  Val accuracy: 59.8214%  Loss: 1.171117\n",
      "Epoch: 16  Val accuracy: 59.3750%  Loss: 1.157905\n",
      "Epoch: 17  Val accuracy: 58.9286%  Loss: 1.145715\n",
      "Epoch: 18  Val accuracy: 61.4955%  Loss: 1.111366\n",
      "Epoch: 19  Val accuracy: 61.9420%  Loss: 1.099729\n",
      "Epoch: 20  Val accuracy: 62.7232%  Loss: 1.085197\n",
      "Epoch: 21  Val accuracy: 63.0580%  Loss: 1.062238\n",
      "Epoch: 22  Val accuracy: 63.5045%  Loss: 1.056351\n",
      "Epoch: 23  Val accuracy: 62.3884%  Loss: 1.046783\n",
      "Epoch: 24  Val accuracy: 63.9509%  Loss: 1.030072\n",
      "Epoch: 25  Val accuracy: 63.0580%  Loss: 1.049029\n",
      "Epoch: 26  Val accuracy: 64.7321%  Loss: 1.025145\n",
      "Epoch: 27  Val accuracy: 65.5134%  Loss: 1.021784\n",
      "Epoch: 28  Val accuracy: 64.7321%  Loss: 1.029677\n",
      "Epoch: 29  Val accuracy: 64.2857%  Loss: 1.029555\n",
      "Epoch: 30  Val accuracy: 65.7366%  Loss: 1.024442\n",
      "Epoch: 31  Val accuracy: 64.6205%  Loss: 1.021211\n",
      "Epoch: 32  Val accuracy: 64.9554%  Loss: 1.018892\n",
      "Epoch: 33  Val accuracy: 64.9554%  Loss: 1.013473\n",
      "Epoch: 34  Val accuracy: 65.9598%  Loss: 1.031214\n",
      "Epoch: 35  Val accuracy: 64.8438%  Loss: 1.000497\n",
      "Epoch: 36  Val accuracy: 66.2946%  Loss: 1.020943\n",
      "Epoch: 37  Val accuracy: 65.0670%  Loss: 1.085791\n",
      "Epoch: 38  Val accuracy: 65.7366%  Loss: 1.070337\n",
      "Epoch: 39  Val accuracy: 64.6205%  Loss: 1.082486\n",
      "Epoch: 40  Val accuracy: 65.7366%  Loss: 1.049311\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs = 40\n",
    "batch_size = 128\n",
    "num_batch = len(x_train) // batch_size\n",
    "num_batch_val = len(x_val) // batch_size\n",
    "\n",
    "full_precision_values = {}  # values fed to the ABC model\n",
    "cur_model_name = 'ResNet20_full'\n",
    "pre_trained_model = None\n",
    "\n",
    "with tf.Session(graph=full_precision_graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(graph_init)\n",
    "    \n",
    "    if pre_trained_model is not None:\n",
    "        try:\n",
    "            print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "            saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Load model Failed!\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(1, num_batch + 1):\n",
    "            X_batch, y_batch = x_train[(iteration-1)*batch_size:iteration*batch_size], y_train[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "            _, acc_train, loss_train = sess.run([train_step, accuracy, cross_entropy],\n",
    "                                                feed_dict={x: X_batch, y: y_batch, keep_prob: 0.8})\n",
    "            \n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%) Train Acc: {:.4f}%\".format(iteration,\n",
    "                                                                           num_batch,\n",
    "                                                                           iteration*100/num_batch,\n",
    "                                                                           acc_train*100),\n",
    "                  end=\"\")\n",
    "        \n",
    "        # At the end of each epoch, measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, num_batch_val + 1):\n",
    "            X_batch, y_batch = x_val[(iteration-1)*batch_size:iteration*batch_size], y_val[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            acc_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, num_batch_val,iteration * 100/num_batch_val),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(epoch + 1, acc_val * 100, loss_val))\n",
    "    \n",
    "    # Save model to file\n",
    "    saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "    \n",
    "    # On completion of training, save the variables to be fed to ABC model\n",
    "    for var_name in full_precision_variables:\n",
    "        full_precision_values[var_name] = sess.run(full_precision_variables[var_name])\n",
    "    \n",
    "    # Save weights to file\n",
    "    f = open(\"Weights/\"+cur_model_name+\".pkl\", \"wb\")\n",
    "    pickle.dump(full_precision_values, f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Full-Precision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cur_model_name = 'ResNet20_full'\n",
    "f = open(\"Weights/\"+cur_model_name+\".pkl\",\"rb\")\n",
    "full_precision_values = pickle.load(f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph of ResNet20 with ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ResNet20 graph with ABC, M = 3, N = 1\n",
    "num_res_blocks = 3\n",
    "ABC_graph_31 = tf.Graph()\n",
    "with ABC_graph_31.as_default():\n",
    "    # Defining inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Store alpha training and alphas\n",
    "    alpha_training_list = []\n",
    "    alphas_list = []\n",
    "    \n",
    "    # Set ABC hyperparameters\n",
    "    M = 3\n",
    "    N = 1\n",
    "    shift_para = tf.Variable(tf.constant(0., shape=(N,1)), dtype=tf.float32, name=\"shift_para\")\n",
    "    betas = tf.Variable(tf.constant(1., shape=(N,1)), dtype=tf.float32, name=\"betas\")\n",
    "    \n",
    "    # Create a variable dict to save weights\n",
    "    ABC_variables = {}\n",
    "    \n",
    "    # Process inputs into the required form\n",
    "    num_filters_out = 16\n",
    "    \n",
    "    with tf.variable_scope('resnet_block', reuse=False):\n",
    "        # Record layer index\n",
    "        i = 1\n",
    "        j = 1\n",
    "        \n",
    "        # Convolution Layer 1 before the resnet block\n",
    "        W_conv1 = tf.Variable(full_precision_values[\"W_conv1\"], name=\"W_conv1\")\n",
    "        b_conv1 = tf.Variable(full_precision_values[\"b_conv1\"], name=\"b_conv1\")\n",
    "        # ABC\n",
    "        alphas1 = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name=\"alphas1\")\n",
    "        binary_weights1 = calculate_binary_weights(W_conv1, M)\n",
    "        alpha_loss1, alpha_training1 = calculate_alphas(W_conv1, binary_weights1, alphas1, M)\n",
    "        h_conv1 = ABC_layer(x_image, binary_weights1, alphas1, shift_para, betas, M, N, b_conv1)\n",
    "        # Save alpha training and alphas\n",
    "        alpha_training_list.append(alpha_training1)\n",
    "        alphas_list.append(alphas1)\n",
    "        \n",
    "        bn_conv1 = tf.layers.batch_normalization(h_conv1, axis=-1, name=\"bn_conv1\")\n",
    "        h_conv1 = tf.nn.relu(bn_conv1)\n",
    "        \n",
    "        ABC_variables['W_conv1'] = W_conv1\n",
    "        ABC_variables['b_conv1'] = b_conv1\n",
    "        \n",
    "        # Start resnet block\n",
    "        i += 1\n",
    "        \n",
    "        for stack in range(3):\n",
    "            for res_block in range(num_res_blocks):\n",
    "                \n",
    "                strides_in = 1\n",
    "                num_filters_in = num_filters_out\n",
    "                num_filters_initial_block = num_filters_in\n",
    "                \n",
    "                #--------------block layer: y path - first y\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = 2  # downsample\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                globals()['W_conv' + str(i)] = tf.Variable(full_precision_values['W_conv' + str(i)], name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = tf.Variable(full_precision_values['b_conv' + str(i)], name='b_conv' + str(i))\n",
    "                # ABC\n",
    "                globals()['alphas' + str(i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(i))\n",
    "                globals()['binary_weights' + str(i)] = calculate_binary_weights(globals()['W_conv' + str(i)], M)\n",
    "                globals()['alpha_loss' + str(i)], globals()['alpha_training' + str(i)] = calculate_alphas(globals()['W_conv' + str(i)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], M)\n",
    "                globals()['conv' + str(i)] = ABC_layer(globals()['h_conv' + str(i-1)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], shift_para, betas, M, N, globals()['b_conv' + str(i)], strides_in)\n",
    "                # save alpha training and alphas\n",
    "                alpha_training_list.append(globals()['alpha_training' + str(i)])\n",
    "                alphas_list.append(globals()['alphas' + str(i)])\n",
    "                \n",
    "                globals()['bn_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                globals()['h_conv' + str(i)] = tf.nn.relu(globals()['bn_conv' + str(i)])\n",
    "                \n",
    "                ABC_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                ABC_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                num_filters_in = num_filters_out\n",
    "                \n",
    "                #--------------block layer: y path - second y\n",
    "                globals()['W_conv' + str(i)] = tf.Variable(full_precision_values['W_conv' + str(i)], name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = tf.Variable(full_precision_values['b_conv' + str(i)], name='b_conv' + str(i))\n",
    "                # ABC\n",
    "                globals()['alphas' + str(i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(i))\n",
    "                globals()['binary_weights' + str(i)] = calculate_binary_weights(globals()['W_conv' + str(i)], M)\n",
    "                globals()['alpha_loss' + str(i)], globals()['alpha_training' + str(i)] = calculate_alphas(globals()['W_conv' + str(i)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], M)\n",
    "                globals()['conv' + str(i)] = ABC_layer(globals()['h_conv' + str(i-1)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], shift_para, betas, M, N, globals()['b_conv' + str(i)])\n",
    "                # save alpha training and alphas\n",
    "                alpha_training_list.append(globals()['alpha_training' + str(i)])\n",
    "                alphas_list.append(globals()['alphas' + str(i)])\n",
    "                \n",
    "                globals()['h_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                \n",
    "                ABC_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                ABC_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                #--------------block layer: x path\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = 2  # downsample\n",
    "                    num_filters_in = num_filters_initial_block\n",
    "                    num_filters_out = num_filters_in * 2 \n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    globals()['W_conv_cut' + str(j)] = tf.Variable(full_precision_values['W_conv_cut' + str(j)], name='W_conv_cut' + str(j))\n",
    "                    globals()['b_conv_cut' + str(j)] = tf.Variable(full_precision_values['b_conv_cut' + str(j)], name='b_conv_cut' + str(j))\n",
    "                    # ABC\n",
    "                    globals()['alphas' + str(j*100+i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(j*100+i))\n",
    "                    globals()['binary_weights' + str(j*100+i)] = calculate_binary_weights(globals()['W_conv_cut' + str(j)], M)\n",
    "                    globals()['alpha_loss' + str(j*100+i)], globals()['alpha_training' + str(j*100+i)] = calculate_alphas(globals()['W_conv_cut' + str(j)], globals()['binary_weights' + str(j*100+i)], globals()['alphas' + str(j*100+i)], M)\n",
    "                    globals()['h_conv_cut' + str(j)] = ABC_layer(globals()['h_conv' + str(i-3)], globals()['binary_weights' + str(j*100+i)], globals()['alphas' + str(j*100+i)], shift_para, betas, M, N, globals()['b_conv_cut' + str(j)], strides_in)\n",
    "                    # save alpha training and alphas\n",
    "                    alpha_training_list.append(globals()['alpha_training' + str(j*100+i)])\n",
    "                    alphas_list.append(globals()['alphas' + str(j*100+i)])\n",
    "                    \n",
    "                    ABC_variables['W_conv_cut' + str(j)] = globals()['W_conv_cut' + str(j)]\n",
    "                    ABC_variables['b_conv_cut' + str(j)] = globals()['b_conv_cut' + str(j)]\n",
    "                    \n",
    "                else:\n",
    "                    globals()['h_conv_cut' + str(j)] = globals()['h_conv' + str(i-3)]\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                #--------------block: sum and acitvate\n",
    "                h_add = tf.add(globals()['h_conv_cut' + str(j-1)] , globals()['h_conv' + str(i-1)])\n",
    "                h_add = tf.nn.relu(h_add)\n",
    "        \n",
    "        # Average pooling\n",
    "        h_pool = tf.nn.max_pool(h_add, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "        \n",
    "        # Flaten the h_add output\n",
    "        h_add_flat = tf.reshape(h_pool, shape=(-1, 2*2*64))\n",
    "        \n",
    "        # Dense layer1\n",
    "        W_fc1 = tf.Variable(full_precision_values[\"W_fc1\"], name=\"W_fc1\")\n",
    "        b_fc1 = tf.Variable(full_precision_values[\"b_fc1\"], name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_add_flat, W_fc1) + b_fc1)\n",
    "        ABC_variables['W_fc1'] = W_fc1\n",
    "        ABC_variables['b_fc1'] = b_fc1\n",
    "        \n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        W_fc2 = tf.Variable(full_precision_values[\"W_fc2\"], name=\"W_fc2\")\n",
    "        b_fc2 = tf.Variable(full_precision_values[\"b_fc2\"], name=\"b_fc2\")\n",
    "        ABC_variables['W_fc2'] = W_fc2\n",
    "        ABC_variables['b_fc2'] = b_fc2\n",
    "        \n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        # Labels\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        y_ = tf.one_hot(y, 10)\n",
    "        \n",
    "        # Defining optimizer and loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Initializer\n",
    "        graph_init = tf.global_variables_initializer()\n",
    "        alphas_init = tf.variables_initializer(alphas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet20 with ABC\n",
    "#### Save the model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 15.5134%  Loss: 2.274299\n",
      "Epoch: 2  Val accuracy: 20.5357%  Loss: 2.106125\n",
      "Epoch: 3  Val accuracy: 21.6518%  Loss: 2.134119\n",
      "Epoch: 4  Val accuracy: 23.2143%  Loss: 2.132984\n",
      "Epoch: 5  Val accuracy: 24.7768%  Loss: 2.136665\n",
      "Epoch: 6  Val accuracy: 22.0982%  Loss: 2.154537\n",
      "Epoch: 7  Val accuracy: 22.8795%  Loss: 2.108738\n",
      "Epoch: 8  Val accuracy: 24.8884%  Loss: 2.091083\n",
      "Epoch: 9  Val accuracy: 24.2188%  Loss: 2.092215\n",
      "Epoch: 10  Val accuracy: 18.5268%  Loss: 2.190234\n",
      "Epoch: 11  Val accuracy: 26.2277%  Loss: 2.047970\n",
      "Epoch: 12  Val accuracy: 23.9955%  Loss: 2.087978\n",
      "Epoch: 13  Val accuracy: 21.9866%  Loss: 2.099424\n",
      "Epoch: 14  Val accuracy: 25.1116%  Loss: 2.068542\n",
      "Epoch: 15  Val accuracy: 19.6429%  Loss: 2.276505\n",
      "Epoch: 16  Val accuracy: 23.5491%  Loss: 2.058861\n",
      "Epoch: 17  Val accuracy: 24.8884%  Loss: 2.095440\n",
      "Epoch: 18  Val accuracy: 24.1071%  Loss: 2.081990\n",
      "Epoch: 19  Val accuracy: 27.3438%  Loss: 2.031139\n",
      "Epoch: 20  Val accuracy: 25.7812%  Loss: 2.085120\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "num_batch = len(x_train) // batch_size\n",
    "num_batch_val = len(x_val) // batch_size\n",
    "num_alpha_epochs = 200\n",
    "\n",
    "ABC_values = {}  # values fed to the ABC model\n",
    "cur_model_name = 'ResNet20_ABC_3_1'\n",
    "pre_trained_model = None\n",
    "\n",
    "loss_history_31=[]\n",
    "\n",
    "with tf.Session(graph=ABC_graph_31) as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(graph_init)\n",
    "    \n",
    "    if pre_trained_model is not None:\n",
    "        try:\n",
    "            print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "            saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Load model Failed!\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(1, num_batch + 1):\n",
    "            X_batch, y_batch = x_train[(iteration-1)*batch_size:iteration*batch_size], y_train[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            \n",
    "            # Training alphas\n",
    "            sess.run(alphas_init)\n",
    "            for alpha_train in alpha_training_list:\n",
    "                for alpha_epoch in range(num_alpha_epochs):\n",
    "                    sess.run(alpha_train)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "            _, acc_train, loss_train = sess.run([train_step, accuracy, cross_entropy],\n",
    "                                                feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            \n",
    "            loss_history_31.append(loss_train)\n",
    "            \n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%) Train Acc: {:.4f}%\".format(iteration,\n",
    "                                                                           num_batch,\n",
    "                                                                           iteration*100/num_batch,\n",
    "                                                                           acc_train*100),\n",
    "                  end=\"\")\n",
    "        \n",
    "        # At the end of each epoch, measure the validation loss and accuracy:\n",
    "        # Training alphas\n",
    "        sess.run(alphas_init)\n",
    "        for alpha_train in alpha_training_list:\n",
    "            for alpha_epoch in range(num_alpha_epochs):\n",
    "                sess.run(alpha_train)\n",
    "        \n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, num_batch_val + 1):\n",
    "            X_batch, y_batch = x_val[(iteration-1)*batch_size:iteration*batch_size], y_val[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            acc_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, num_batch_val,iteration * 100/num_batch_val),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(epoch + 1, acc_val * 100, loss_val))\n",
    "    \n",
    "    # Save model to file\n",
    "    saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "    \n",
    "    # On completion of training, save the variables to be fed to ABC model\n",
    "    for var_name in ABC_variables:\n",
    "        ABC_values[var_name] = sess.run(ABC_variables[var_name])\n",
    "    \n",
    "    # Save weights to file\n",
    "    f = open(\"Weights/\"+cur_model_name+\".pkl\", \"wb\")\n",
    "    pickle.dump(ABC_values, f)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
