{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Group Project\n",
    "\n",
    "## Topic: Towards Accurate Binary Convolutional Neural Network\n",
    "## Group Member: Qichen Hu, Xuechun Zhang, Yingtong Han\n",
    "\n",
    "In this project, we replicated the results in Towards Accurate Binary Convolutional Neural Network [Lin et al., 2017]. The core idea is to approximate the weights of convolution layer by binarized weights for acceleration purpose. We built the model based on tensorflow 1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO') # disable warnings\n",
    "import numpy as np\n",
    "\n",
    "# import utils functions\n",
    "from utils_functions import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (49000, 32, 32, 3)\n",
      "x_val shape: (1000, 32, 32, 3)\n",
      "y_train shape: (49000, 1)\n",
      "y_val shape: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load cifar10 data\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "# scale data to [0,1]\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# train validation split\n",
    "x_train, x_val = x_train[:-1000], x_train[-1000:]\n",
    "y_train, y_val = y_train[:-1000], y_train[-1000:]\n",
    "\n",
    "# check shape\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('x_val shape:',x_val.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('y_val shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph of Traditional ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the full-precision ResNet34 graph without ABC\n",
    "num_res_blocks_list = [3,4,6,3]\n",
    "full_precision_graph = tf.Graph()\n",
    "with full_precision_graph.as_default():\n",
    "    # Defining inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Regularization term\n",
    "    regularizer = 0\n",
    "    reg = 0.01\n",
    "    \n",
    "    # Create a variable dict to save weights\n",
    "    full_precision_variables = {}\n",
    "    \n",
    "    # Process inputs into the required form\n",
    "    num_filters_out = 16\n",
    "    \n",
    "    with tf.variable_scope('resnet_block', reuse=False):\n",
    "        # Record layer index\n",
    "        i = 1\n",
    "        j = 1\n",
    "        \n",
    "        # Convolution Layer 1 before the resnet block\n",
    "        W_conv1 = weight_variable(shape=([3,3,3,num_filters_out]), name=\"W_conv1\")  #kernel size=3, input channels=3\n",
    "        b_conv1 = bias_variable(shape=[num_filters_out], name=\"b_conv1\")\n",
    "        conv1 = (conv2d(x_image, W_conv1) + b_conv1)\n",
    "        #print(x_image.get_shape(), W_conv1.get_shape())\n",
    "        bn_conv1 = tf.layers.batch_normalization(conv1, axis=-1, name=\"bn_conv1\")\n",
    "        h_conv1 = tf.nn.relu(bn_conv1)\n",
    "        \n",
    "        full_precision_variables['W_conv1'] = W_conv1\n",
    "        full_precision_variables['b_conv1'] = b_conv1\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_conv1)\n",
    "        \n",
    "        # Start resnet block\n",
    "        i += 1\n",
    "        \n",
    "        for stack in range(4):\n",
    "            for res_block in range(num_res_blocks_list[stack]):\n",
    "                \n",
    "                strides_in = [1,1,1,1]\n",
    "                num_filters_in = num_filters_out\n",
    "                num_filters_initial_block = num_filters_in\n",
    "                \n",
    "                #--------------block layer: y path - first y\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = [1,2,2,1]  # downsample\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                globals()['W_conv' + str(i)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = bias_variable(shape=[num_filters_out], name='b_conv' + str(i)) \n",
    "                globals()['conv' + str(i)] = (conv2d(globals()['h_conv' + str(i-1)], globals()['W_conv' + str(i)], strides_in) + globals()['b_conv' + str(i)])\n",
    "                #print(globals()['h_conv' + str(i-1)].get_shape(), globals()['W_conv' + str(i)].get_shape())\n",
    "                globals()['bn_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                globals()['h_conv' + str(i)] = tf.nn.relu(globals()['bn_conv' + str(i)])\n",
    "                full_precision_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                full_precision_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                regularizer += tf.nn.l2_loss(globals()['W_conv' + str(i)])\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                num_filters_in = num_filters_out\n",
    "                \n",
    "                #--------------block layer: y path - second y\n",
    "                globals()['W_conv' + str(i)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = bias_variable(shape=[num_filters_out], name='b_conv' + str(i)) \n",
    "                globals()['conv' + str(i)] = (conv2d(globals()['h_conv' + str(i-1)], globals()['W_conv' + str(i)]) + globals()['b_conv' + str(i)])\n",
    "                #print(globals()['h_conv' + str(i-1)].get_shape(), globals()['W_conv' + str(i)].get_shape())\n",
    "                globals()['h_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                full_precision_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                full_precision_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                regularizer += tf.nn.l2_loss(globals()['W_conv' + str(i)])\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                #--------------block layer: x path\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = [1,2,2,1]  # downsample\n",
    "                    num_filters_in = num_filters_initial_block\n",
    "                    num_filters_out = num_filters_in * 2 \n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    globals()['W_conv_cut' + str(j)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv_cut' + str(j))\n",
    "                    globals()['b_conv_cut' + str(j)] = bias_variable(shape=[num_filters_out], name='b_conv_cut' + str(j))\n",
    "                    globals()['h_conv_cut' + str(j)] = (conv2d(globals()['h_conv' + str(i-3)], globals()['W_conv_cut' + str(j)], strides_in) + globals()['b_conv_cut' + str(j)])\n",
    "                    #print('x',globals()['h_conv' + str(i-3)].get_shape(), globals()['W_conv_cut' + str(j)].get_shape())\n",
    "                    full_precision_variables['W_conv_cut' + str(j)] = globals()['W_conv_cut' + str(j)]\n",
    "                    full_precision_variables['b_conv_cut' + str(j)] = globals()['b_conv_cut' + str(j)]\n",
    "                    \n",
    "                    regularizer += tf.nn.l2_loss(globals()['W_conv_cut' + str(j)])\n",
    "                    \n",
    "                else:\n",
    "                    globals()['h_conv_cut' + str(j)] = globals()['h_conv' + str(i-3)]\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                #--------------block: sum and acitvate\n",
    "                h_add = tf.add(globals()['h_conv_cut' + str(j-1)] , globals()['h_conv' + str(i-1)])\n",
    "                h_add = tf.nn.relu(h_add)\n",
    "        \n",
    "        # Average pooling\n",
    "        h_pool = tf.nn.max_pool(h_add, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "        \n",
    "        # Flaten the h_add output\n",
    "        h_add_flat = tf.reshape(h_pool, shape=(-1, 1*1*128))\n",
    "\n",
    "        # Dense layer1\n",
    "        W_fc1 = weight_variable(shape=[1*1*128, 1024], name=\"W_fc1\")\n",
    "        b_fc1 = bias_variable(shape=[1024], name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_add_flat, W_fc1) + b_fc1)\n",
    "        full_precision_variables['W_fc1'] = W_fc1\n",
    "        full_precision_variables['b_fc1'] = b_fc1\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_fc1)\n",
    "        \n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        W_fc2 = weight_variable(shape=[1024, 10], name=\"W_fc2\")\n",
    "        b_fc2 = bias_variable(shape=[10], name=\"b_fc2\")\n",
    "        full_precision_variables['W_fc2'] = W_fc2\n",
    "        full_precision_variables['b_fc2'] = b_fc2\n",
    "        \n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_fc2)\n",
    "        \n",
    "        # Labels\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        y_ = tf.one_hot(y, 10)\n",
    "\n",
    "        # Defining optimizer and loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "        train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy + reg*regularizer)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Initializer\n",
    "        graph_init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet34\n",
    "#### Save the model and weights for initialization of ABC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 10.2679%  Loss: 4.186936\n",
      "Epoch: 2  Val accuracy: 13.8393%  Loss: 2.320942\n",
      "Epoch: 3  Val accuracy: 15.8482%  Loss: 2.259161\n",
      "Epoch: 4  Val accuracy: 17.0759%  Loss: 2.199602\n",
      "Epoch: 5  Val accuracy: 19.0848%  Loss: 2.119938\n",
      "Epoch: 6  Val accuracy: 23.5491%  Loss: 2.070393\n",
      "Epoch: 7  Val accuracy: 21.3170%  Loss: 2.034542\n",
      "Epoch: 8  Val accuracy: 24.6652%  Loss: 2.000831\n",
      "Epoch: 9  Val accuracy: 24.1071%  Loss: 1.987054\n",
      "Epoch: 10  Val accuracy: 26.7857%  Loss: 1.938736\n",
      "Epoch: 11  Val accuracy: 29.6875%  Loss: 1.894928\n",
      "Epoch: 12  Val accuracy: 27.6786%  Loss: 1.882040\n",
      "Epoch: 13  Val accuracy: 31.0268%  Loss: 1.879210\n",
      "Epoch: 14  Val accuracy: 30.2455%  Loss: 1.874845\n",
      "Epoch: 15  Val accuracy: 35.2679%  Loss: 1.801635\n",
      "Epoch: 16  Val accuracy: 34.7098%  Loss: 1.785804\n",
      "Epoch: 17  Val accuracy: 35.3795%  Loss: 1.763273\n",
      "Epoch: 18  Val accuracy: 36.4955%  Loss: 1.740854\n",
      "Epoch: 19  Val accuracy: 38.5045%  Loss: 1.702831\n",
      "Epoch: 20  Val accuracy: 38.9509%  Loss: 1.685941\n",
      "Epoch: 21  Val accuracy: 39.7321%  Loss: 1.640635\n",
      "Epoch: 22  Val accuracy: 41.5179%  Loss: 1.628481\n",
      "Epoch: 23  Val accuracy: 40.8482%  Loss: 1.628101\n",
      "Epoch: 24  Val accuracy: 43.6384%  Loss: 1.570231\n",
      "Epoch: 25  Val accuracy: 42.4107%  Loss: 1.576401\n",
      "Epoch: 26  Val accuracy: 47.5446%  Loss: 1.527529\n",
      "Epoch: 27  Val accuracy: 45.4241%  Loss: 1.492434\n",
      "Epoch: 28  Val accuracy: 45.7589%  Loss: 1.472921\n",
      "Epoch: 29  Val accuracy: 46.0938%  Loss: 1.462832\n",
      "Epoch: 30  Val accuracy: 48.6607%  Loss: 1.421746\n",
      "Epoch: 31  Val accuracy: 48.5491%  Loss: 1.411710\n",
      "Epoch: 32  Val accuracy: 47.3214%  Loss: 1.440312\n",
      "Epoch: 33  Val accuracy: 47.8795%  Loss: 1.429223\n",
      "Epoch: 34  Val accuracy: 49.6652%  Loss: 1.413496\n",
      "Epoch: 35  Val accuracy: 51.0045%  Loss: 1.368194\n",
      "Epoch: 36  Val accuracy: 49.7768%  Loss: 1.409432\n",
      "Epoch: 37  Val accuracy: 50.5580%  Loss: 1.340658\n",
      "Epoch: 38  Val accuracy: 51.7857%  Loss: 1.316141\n",
      "Epoch: 39  Val accuracy: 53.4598%  Loss: 1.284576\n",
      "Epoch: 40  Val accuracy: 53.2366%  Loss: 1.337719\n",
      "Epoch: 41  Val accuracy: 54.7991%  Loss: 1.284506\n",
      "Epoch: 42  Val accuracy: 54.3527%  Loss: 1.250672\n",
      "Epoch: 43  Val accuracy: 53.0134%  Loss: 1.354755\n",
      "Epoch: 44  Val accuracy: 52.3438%  Loss: 1.323795\n",
      "Epoch: 45  Val accuracy: 56.4732%  Loss: 1.250268\n",
      "Epoch: 46  Val accuracy: 55.1339%  Loss: 1.232583\n",
      "Epoch: 47  Val accuracy: 57.5893%  Loss: 1.199819\n",
      "Epoch: 48  Val accuracy: 57.2545%  Loss: 1.221618\n",
      "Epoch: 49  Val accuracy: 57.4777%  Loss: 1.226994\n",
      "Epoch: 50  Val accuracy: 59.8214%  Loss: 1.131628\n",
      "Epoch: 51  Val accuracy: 57.1429%  Loss: 1.184278\n",
      "Epoch: 52  Val accuracy: 60.4911%  Loss: 1.124638\n",
      "Epoch: 53  Val accuracy: 60.3795%  Loss: 1.154755\n",
      "Epoch: 54  Val accuracy: 60.8259%  Loss: 1.134432\n",
      "Epoch: 55  Val accuracy: 59.4866%  Loss: 1.170486\n",
      "Epoch: 56  Val accuracy: 60.6027%  Loss: 1.097736\n",
      "Epoch: 57  Val accuracy: 61.3839%  Loss: 1.091092\n",
      "Epoch: 58  Val accuracy: 61.4955%  Loss: 1.090274\n",
      "Epoch: 59  Val accuracy: 60.3795%  Loss: 1.140072\n",
      "Epoch: 60  Val accuracy: 57.5893%  Loss: 1.219520\n",
      "Epoch: 61  Val accuracy: 60.2679%  Loss: 1.210791\n",
      "Epoch: 62  Val accuracy: 57.1429%  Loss: 1.203178\n",
      "Epoch: 63  Val accuracy: 58.5938%  Loss: 1.174343\n",
      "Epoch: 64  Val accuracy: 61.3839%  Loss: 1.133983\n",
      "Epoch: 65  Val accuracy: 62.8348%  Loss: 1.065929\n",
      "Epoch: 66  Val accuracy: 59.1518%  Loss: 1.121970\n",
      "Epoch: 67  Val accuracy: 61.4955%  Loss: 1.104559\n",
      "Epoch: 68  Val accuracy: 61.6071%  Loss: 1.133951\n",
      "Epoch: 69  Val accuracy: 62.1652%  Loss: 1.054037\n",
      "Epoch: 70  Val accuracy: 64.7321%  Loss: 1.044840\n",
      "Epoch: 71  Val accuracy: 65.4018%  Loss: 0.993663\n",
      "Epoch: 72  Val accuracy: 62.7232%  Loss: 1.077714\n",
      "Epoch: 73  Val accuracy: 64.6205%  Loss: 1.036508\n",
      "Epoch: 74  Val accuracy: 66.4062%  Loss: 0.997002\n",
      "Epoch: 75  Val accuracy: 65.1786%  Loss: 0.995249\n",
      "Epoch: 76  Val accuracy: 64.7321%  Loss: 1.048477\n",
      "Epoch: 77  Val accuracy: 64.3973%  Loss: 1.030909\n",
      "Epoch: 78  Val accuracy: 64.1741%  Loss: 1.023585\n",
      "Epoch: 79  Val accuracy: 64.5089%  Loss: 1.022170\n",
      "Epoch: 80  Val accuracy: 62.6116%  Loss: 1.024267\n",
      "Epoch: 81  Val accuracy: 65.7366%  Loss: 0.977949\n",
      "Epoch: 82  Val accuracy: 65.4018%  Loss: 0.993029\n",
      "Epoch: 83  Val accuracy: 63.8393%  Loss: 1.017072\n",
      "Epoch: 84  Val accuracy: 65.0670%  Loss: 0.980232\n",
      "Epoch: 85  Val accuracy: 66.0714%  Loss: 0.968760\n",
      "Epoch: 86  Val accuracy: 65.6250%  Loss: 0.967058\n",
      "Epoch: 87  Val accuracy: 64.7321%  Loss: 1.010036\n",
      "Epoch: 88  Val accuracy: 65.6250%  Loss: 0.986171\n",
      "Epoch: 89  Val accuracy: 65.0670%  Loss: 0.998715\n",
      "Epoch: 90  Val accuracy: 67.0759%  Loss: 0.957843\n",
      "Epoch: 91  Val accuracy: 66.9643%  Loss: 0.944131\n",
      "Epoch: 92  Val accuracy: 67.5223%  Loss: 0.952673\n",
      "Epoch: 93  Val accuracy: 66.0714%  Loss: 0.973441\n",
      "Epoch: 94  Val accuracy: 66.2946%  Loss: 0.927851\n",
      "Epoch: 95  Val accuracy: 66.1830%  Loss: 0.958639\n",
      "Epoch: 96  Val accuracy: 66.8527%  Loss: 0.916216\n",
      "Epoch: 97  Val accuracy: 67.8571%  Loss: 0.926751\n",
      "Epoch: 98  Val accuracy: 67.8571%  Loss: 0.925276\n",
      "Epoch: 99  Val accuracy: 66.2946%  Loss: 0.968107\n",
      "Epoch: 100  Val accuracy: 68.1920%  Loss: 0.892464\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "num_batch = len(x_train) // batch_size\n",
    "num_batch_val = len(x_val) // batch_size\n",
    "\n",
    "full_precision_values = {}  # values fed to the ABC model\n",
    "cur_model_name = 'ResNet34_full'\n",
    "pre_trained_model = None\n",
    "\n",
    "with tf.Session(graph=full_precision_graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(graph_init)\n",
    "    \n",
    "    if pre_trained_model is not None:\n",
    "        try:\n",
    "            print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "            saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Load model Failed!\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(1, num_batch + 1):\n",
    "            X_batch, y_batch = x_train[(iteration-1)*batch_size:iteration*batch_size], y_train[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "            _, acc_train, loss_train = sess.run([train_step, accuracy, cross_entropy],\n",
    "                                                feed_dict={x: X_batch, y: y_batch, keep_prob: 0.5})\n",
    "            \n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%) Train Acc: {:.4f}%\".format(iteration,\n",
    "                                                                           num_batch,\n",
    "                                                                           iteration*100/num_batch,\n",
    "                                                                           acc_train*100),\n",
    "                  end=\"\")\n",
    "        \n",
    "        # At the end of each epoch, measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, num_batch_val + 1):\n",
    "            X_batch, y_batch = x_val[(iteration-1)*batch_size:iteration*batch_size], y_val[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            acc_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, num_batch_val,iteration * 100/num_batch_val),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(epoch + 1, acc_val * 100, loss_val))\n",
    "    \n",
    "    # Save model to file\n",
    "    saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "    \n",
    "    # On completion of training, save the variables to be fed to ABC model\n",
    "    for var_name in full_precision_variables:\n",
    "        full_precision_values[var_name] = sess.run(full_precision_variables[var_name])\n",
    "    \n",
    "    # Save weights to file\n",
    "    f = open(\"Weights/\"+cur_model_name+\".pkl\", \"wb\")\n",
    "    pickle.dump(full_precision_values, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph of ResNet34 with ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ResNet34 graph with ABC, M = 5, N = 5\n",
    "num_res_blocks_list = [3,4,6,3]\n",
    "ABC_graph = tf.Graph()\n",
    "with ABC_graph.as_default():\n",
    "    # Defining inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Store alpha training and alphas\n",
    "    alpha_training_list = []\n",
    "    alphas_list = []\n",
    "    \n",
    "    # Set ABC hyperparameters\n",
    "    M = 5\n",
    "    N = 5\n",
    "    s_min, s_max = -0.2, 0.2\n",
    "    s_gap = (s_max - s_min) / (N-1)\n",
    "    shift_para = tf.Variable(tf.constant(np.arange(s_min, s_max+1e-4, s_gap).tolist(), dtype=tf.float32, name=\"shift_para\"))\n",
    "    betas = tf.Variable(tf.constant(1/N, shape=(N,1)), dtype=tf.float32, name=\"betas\")\n",
    "    \n",
    "    # Create a variable dict to save weights\n",
    "    ABC_variables = {}\n",
    "    \n",
    "    # Process inputs into the required form\n",
    "    num_filters_out = 16\n",
    "    \n",
    "    with tf.variable_scope('resnet_block', reuse=False):\n",
    "        # Record layer index\n",
    "        i = 1\n",
    "        j = 1\n",
    "        \n",
    "        # Convolution Layer 1 before the resnet block\n",
    "        W_conv1 = tf.Variable(full_precision_values[\"W_conv1\"], name=\"W_conv1\")\n",
    "        b_conv1 = tf.Variable(full_precision_values[\"b_conv1\"], name=\"b_conv1\")\n",
    "        # ABC\n",
    "        alphas1 = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name=\"alphas1\")\n",
    "        binary_weights1 = calculate_binary_weights(W_conv1, M)\n",
    "        alpha_loss1, alpha_training1 = calculate_alphas(W_conv1, binary_weights1, alphas1, M)\n",
    "        h_conv1 = ABC_layer(x_image, binary_weights1, alphas1, shift_para, betas, M, N, b_conv1)\n",
    "        # Save alpha training and alphas\n",
    "        alpha_training_list.append(alpha_training1)\n",
    "        alphas_list.append(alphas1)\n",
    "        \n",
    "        bn_conv1 = tf.layers.batch_normalization(h_conv1, axis=-1, name=\"bn_conv1\")\n",
    "        h_conv1 = tf.nn.relu(bn_conv1)\n",
    "        \n",
    "        ABC_variables['W_conv1'] = W_conv1\n",
    "        ABC_variables['b_conv1'] = b_conv1\n",
    "        \n",
    "        # Start resnet block\n",
    "        i += 1\n",
    "        \n",
    "        for stack in range(4):\n",
    "            for res_block in range(num_res_blocks_list[stack]):\n",
    "                \n",
    "                strides_in = 1\n",
    "                num_filters_in = num_filters_out\n",
    "                num_filters_initial_block = num_filters_in\n",
    "                \n",
    "                #--------------block layer: y path - first y\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = 2  # downsample\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                globals()['W_conv' + str(i)] = tf.Variable(full_precision_values['W_conv' + str(i)], name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = tf.Variable(full_precision_values['b_conv' + str(i)], name='b_conv' + str(i))\n",
    "                # ABC\n",
    "                globals()['alphas' + str(i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(i))\n",
    "                globals()['binary_weights' + str(i)] = calculate_binary_weights(globals()['W_conv' + str(i)], M)\n",
    "                globals()['alpha_loss' + str(i)], globals()['alpha_training' + str(i)] = calculate_alphas(globals()['W_conv' + str(i)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], M)\n",
    "                globals()['conv' + str(i)] = ABC_layer(globals()['h_conv' + str(i-1)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], shift_para, betas, M, N, globals()['b_conv' + str(i)], strides_in)\n",
    "                # save alpha training and alphas\n",
    "                alpha_training_list.append(globals()['alpha_training' + str(i)])\n",
    "                alphas_list.append(globals()['alphas' + str(i)])\n",
    "                \n",
    "                globals()['bn_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                globals()['h_conv' + str(i)] = tf.nn.relu(globals()['bn_conv' + str(i)])\n",
    "                \n",
    "                ABC_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                ABC_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                num_filters_in = num_filters_out\n",
    "                \n",
    "                #--------------block layer: y path - second y\n",
    "                globals()['W_conv' + str(i)] = tf.Variable(full_precision_values['W_conv' + str(i)], name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = tf.Variable(full_precision_values['b_conv' + str(i)], name='b_conv' + str(i))\n",
    "                # ABC\n",
    "                globals()['alphas' + str(i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(i))\n",
    "                globals()['binary_weights' + str(i)] = calculate_binary_weights(globals()['W_conv' + str(i)], M)\n",
    "                globals()['alpha_loss' + str(i)], globals()['alpha_training' + str(i)] = calculate_alphas(globals()['W_conv' + str(i)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], M)\n",
    "                globals()['conv' + str(i)] = ABC_layer(globals()['h_conv' + str(i-1)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], shift_para, betas, M, N, globals()['b_conv' + str(i)])\n",
    "                # save alpha training and alphas\n",
    "                alpha_training_list.append(globals()['alpha_training' + str(i)])\n",
    "                alphas_list.append(globals()['alphas' + str(i)])\n",
    "                \n",
    "                globals()['h_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                \n",
    "                ABC_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                ABC_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                #--------------block layer: x path\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = 2  # downsample\n",
    "                    num_filters_in = num_filters_initial_block\n",
    "                    num_filters_out = num_filters_in * 2 \n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    globals()['W_conv_cut' + str(j)] = tf.Variable(full_precision_values['W_conv_cut' + str(j)], name='W_conv_cut' + str(j))\n",
    "                    globals()['b_conv_cut' + str(j)] = tf.Variable(full_precision_values['b_conv_cut' + str(j)], name='b_conv_cut' + str(j))\n",
    "                    # ABC\n",
    "                    globals()['alphas' + str(j*100+i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(j*100+i))\n",
    "                    globals()['binary_weights' + str(j*100+i)] = calculate_binary_weights(globals()['W_conv_cut' + str(j)], M)\n",
    "                    globals()['alpha_loss' + str(j*100+i)], globals()['alpha_training' + str(j*100+i)] = calculate_alphas(globals()['W_conv_cut' + str(j)], globals()['binary_weights' + str(j*100+i)], globals()['alphas' + str(j*100+i)], M)\n",
    "                    globals()['h_conv_cut' + str(j)] = ABC_layer(globals()['h_conv' + str(i-3)], globals()['binary_weights' + str(j*100+i)], globals()['alphas' + str(j*100+i)], shift_para, betas, M, N, globals()['b_conv_cut' + str(j)], strides_in)\n",
    "                    # save alpha training and alphas\n",
    "                    alpha_training_list.append(globals()['alpha_training' + str(j*100+i)])\n",
    "                    alphas_list.append(globals()['alphas' + str(j*100+i)])\n",
    "                    \n",
    "                    ABC_variables['W_conv_cut' + str(j)] = globals()['W_conv_cut' + str(j)]\n",
    "                    ABC_variables['b_conv_cut' + str(j)] = globals()['b_conv_cut' + str(j)]\n",
    "                    \n",
    "                else:\n",
    "                    globals()['h_conv_cut' + str(j)] = globals()['h_conv' + str(i-3)]\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                #--------------block: sum and acitvate\n",
    "                h_add = tf.add(globals()['h_conv_cut' + str(j-1)] , globals()['h_conv' + str(i-1)])\n",
    "                h_add = tf.nn.relu(h_add)\n",
    "        \n",
    "        # Average pooling\n",
    "        h_pool = tf.nn.max_pool(h_add, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "        \n",
    "        # Flaten the h_add output\n",
    "        h_add_flat = tf.reshape(h_pool, shape=(-1, 1*1*128))\n",
    "        \n",
    "        # Dense layer1\n",
    "        W_fc1 = tf.Variable(full_precision_values[\"W_fc1\"], name=\"W_fc1\")\n",
    "        b_fc1 = tf.Variable(full_precision_values[\"b_fc1\"], name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_add_flat, W_fc1) + b_fc1)\n",
    "        ABC_variables['W_fc1'] = W_fc1\n",
    "        ABC_variables['b_fc1'] = b_fc1\n",
    "        \n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        W_fc2 = tf.Variable(full_precision_values[\"W_fc2\"], name=\"W_fc2\")\n",
    "        b_fc2 = tf.Variable(full_precision_values[\"b_fc2\"], name=\"b_fc2\")\n",
    "        ABC_variables['W_fc2'] = W_fc2\n",
    "        ABC_variables['b_fc2'] = b_fc2\n",
    "        \n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        # Labels\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        y_ = tf.one_hot(y, 10)\n",
    "        \n",
    "        # Defining optimizer and loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Initializer\n",
    "        graph_init = tf.global_variables_initializer()\n",
    "        alphas_init = tf.variables_initializer(alphas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet34 with ABC\n",
    "#### Save the model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 10.0446%  Loss: 3.016400\n",
      "Epoch: 2  Val accuracy: 12.9464%  Loss: 2.433650\n",
      "Epoch: 3  Val accuracy: 12.8348%  Loss: 2.368501\n",
      "Epoch: 4  Val accuracy: 16.0714%  Loss: 2.307099\n",
      "Epoch: 5  Val accuracy: 10.4911%  Loss: 2.444845\n",
      "Epoch: 6  Val accuracy: 12.8348%  Loss: 2.309932\n",
      "Epoch: 7  Val accuracy: 15.0670%  Loss: 2.271864\n",
      "Epoch: 8  Val accuracy: 17.9688%  Loss: 2.260310\n",
      "Epoch: 9  Val accuracy: 9.5982%  Loss: 2.346605\n",
      "Epoch: 10  Val accuracy: 12.1652%  Loss: 2.346973\n",
      "Epoch: 11  Val accuracy: 11.4955%  Loss: 2.345157\n",
      "Epoch: 12  Val accuracy: 14.6205%  Loss: 2.266306\n",
      "Epoch: 13  Val accuracy: 12.7232%  Loss: 2.291463\n",
      "Epoch: 14  Val accuracy: 16.1830%  Loss: 2.238191\n",
      "Epoch: 15  Val accuracy: 15.0670%  Loss: 2.238681\n",
      "Epoch: 16  Val accuracy: 15.2902%  Loss: 2.275445\n",
      "Epoch: 17  Val accuracy: 11.2723%  Loss: 2.316804\n",
      "Epoch: 18  Val accuracy: 13.8393%  Loss: 2.245106\n",
      "Epoch: 19  Val accuracy: 15.7366%  Loss: 2.265240\n",
      "Epoch: 20  Val accuracy: 14.8438%  Loss: 2.233760\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "num_batch = len(x_train) // batch_size\n",
    "num_batch_val = len(x_val) // batch_size\n",
    "num_alpha_epochs = 200\n",
    "\n",
    "ABC_values = {}  # values fed to the ABC model\n",
    "cur_model_name = 'ResNet34_ABC_5_5'\n",
    "pre_trained_model = None\n",
    "\n",
    "loss_history=[]\n",
    "\n",
    "with tf.Session(graph=ABC_graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(graph_init)\n",
    "    \n",
    "    if pre_trained_model is not None:\n",
    "        try:\n",
    "            print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "            saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Load model Failed!\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(1, num_batch + 1):\n",
    "            X_batch, y_batch = x_train[(iteration-1)*batch_size:iteration*batch_size], y_train[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            \n",
    "            # Training alphas\n",
    "            sess.run(alphas_init)\n",
    "            for alpha_train in alpha_training_list:\n",
    "                for alpha_epoch in range(num_alpha_epochs):\n",
    "                    sess.run(alpha_train)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "            _, acc_train, loss_train = sess.run([train_step, accuracy, cross_entropy],\n",
    "                                                feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_history.append(loss_train)\n",
    "            \n",
    "            \n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%) Train Acc: {:.4f}%\".format(iteration,\n",
    "                                                                           num_batch,\n",
    "                                                                           iteration*100/num_batch,\n",
    "                                                                           acc_train*100),\n",
    "                  end=\"\")\n",
    "        \n",
    "        # At the end of each epoch, measure the validation loss and accuracy:\n",
    "        # Training alphas\n",
    "        sess.run(alphas_init)\n",
    "        for alpha_train in alpha_training_list:\n",
    "            for alpha_epoch in range(num_alpha_epochs):\n",
    "                sess.run(alpha_train)\n",
    "        \n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, num_batch_val + 1):\n",
    "            X_batch, y_batch = x_val[(iteration-1)*batch_size:iteration*batch_size], y_val[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            acc_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, num_batch_val,iteration * 100/num_batch_val),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(epoch + 1, acc_val * 100, loss_val))\n",
    "    \n",
    "    # Save model to file\n",
    "    saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "    \n",
    "    # On completion of training, save the variables to be fed to ABC model\n",
    "    for var_name in ABC_variables:\n",
    "        ABC_values[var_name] = sess.run(ABC_variables[var_name])\n",
    "    \n",
    "    # Save weights to file\n",
    "    f = open(\"Weights/\"+cur_model_name+\".pkl\", \"wb\")\n",
    "    pickle.dump(ABC_values, f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_history, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
