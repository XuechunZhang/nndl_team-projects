{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Group Project\n",
    "\n",
    "## Topic: Towards Accurate Binary Convolutional Neural Network\n",
    "## Group Member: Qichen Hu, Xuechun Zhang, Yingtong Han\n",
    "\n",
    "In this project, we replicated the results in Towards Accurate Binary Convolutional Neural Network [Lin et al., 2017]. The core idea is to approximate the weights of convolution layer by binarized weights for acceleration purpose. We built the model based on tensorflow 1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO') # disable warnings\n",
    "import numpy as np\n",
    "\n",
    "# import utils functions\n",
    "from utils_functions import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (49000, 32, 32, 3)\n",
      "x_val shape: (1000, 32, 32, 3)\n",
      "y_train shape: (49000, 1)\n",
      "y_val shape: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load cifar10 data\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "\n",
    "# scale data to [0,1]\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# train validation split\n",
    "x_train, x_val = x_train[:-1000], x_train[-1000:]\n",
    "y_train, y_val = y_train[:-1000], y_train[-1000:]\n",
    "\n",
    "# check shape\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('x_val shape:',x_val.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('y_val shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph of Traditional ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-89f1d3fec4c3>:28: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-89f1d3fec4c3>:115: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-89f1d3fec4c3>:132: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the full-precision ResNet34 graph without ABC\n",
    "num_res_blocks = [3, 4, 6, 3]\n",
    "full_precision_graph = tf.Graph()\n",
    "with full_precision_graph.as_default():\n",
    "    # Defining inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Regularization term\n",
    "    regularizer = 0\n",
    "    reg = 0.01\n",
    "    \n",
    "    # Create a variable dict to save weights\n",
    "    full_precision_variables = {}\n",
    "    \n",
    "    # Process inputs into the required form\n",
    "    num_filters_out = 16\n",
    "    \n",
    "    with tf.variable_scope('resnet_block', reuse=False):\n",
    "        # Record layer index\n",
    "        i = 1\n",
    "        j = 1\n",
    "        \n",
    "        # Convolution Layer 1 before the resnet block\n",
    "        W_conv1 = weight_variable(shape=([3,3,3,num_filters_out]), name=\"W_conv1\")  #kernel size=3, input channels=3\n",
    "        b_conv1 = bias_variable(shape=[num_filters_out], name=\"b_conv1\")\n",
    "        conv1 = (conv2d(x_image, W_conv1) + b_conv1)\n",
    "        bn_conv1 = tf.layers.batch_normalization(conv1, axis=-1, name=\"bn_conv1\")\n",
    "        h_conv1 = tf.nn.relu(bn_conv1)\n",
    "        \n",
    "        full_precision_variables['W_conv1'] = W_conv1\n",
    "        full_precision_variables['b_conv1'] = b_conv1\n",
    "        \n",
    "        # Start resnet block\n",
    "        i += 1\n",
    "        \n",
    "        for stack in range(4):\n",
    "            for res_block in range(num_res_blocks[stack]):\n",
    "                \n",
    "                strides_in = [1,1,1,1]\n",
    "                num_filters_in = num_filters_out\n",
    "                num_filters_initial_block = num_filters_in\n",
    "                \n",
    "                #--------------block layer: y path - first y\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = [1,2,2,1]  # downsample\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                globals()['W_conv' + str(i)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = bias_variable(shape=[num_filters_out], name='b_conv' + str(i)) \n",
    "                globals()['conv' + str(i)] = (conv2d(globals()['h_conv' + str(i-1)], globals()['W_conv' + str(i)], strides_in) + globals()['b_conv' + str(i)])\n",
    "                globals()['bn_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                globals()['h_conv' + str(i)] = tf.nn.relu(globals()['bn_conv' + str(i)])\n",
    "                full_precision_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                full_precision_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                regularizer += tf.nn.l2_loss(globals()['W_conv' + str(i)])\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                num_filters_in = num_filters_out\n",
    "                \n",
    "                #--------------block layer: y path - second y\n",
    "                globals()['W_conv' + str(i)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = bias_variable(shape=[num_filters_out], name='b_conv' + str(i)) \n",
    "                globals()['conv' + str(i)] = (conv2d(globals()['h_conv' + str(i-1)], globals()['W_conv' + str(i)]) + globals()['b_conv' + str(i)])\n",
    "                globals()['h_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                full_precision_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                full_precision_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                regularizer += tf.nn.l2_loss(globals()['W_conv' + str(i)])\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                #--------------block layer: x path\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = [1,2,2,1]  # downsample\n",
    "                    num_filters_in = num_filters_initial_block\n",
    "                    num_filters_out = num_filters_in * 2 \n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    globals()['W_conv_cut' + str(j)] = weight_variable(shape=([3,3,num_filters_in,num_filters_out]), name='W_conv_cut' + str(j))\n",
    "                    globals()['b_conv_cut' + str(j)] = bias_variable(shape=[num_filters_out], name='b_conv_cut' + str(j))\n",
    "                    globals()['h_conv_cut' + str(j)] = (conv2d(globals()['h_conv' + str(i-3)], globals()['W_conv_cut' + str(j)], strides_in) + globals()['b_conv_cut' + str(j)])\n",
    "                    full_precision_variables['W_conv_cut' + str(j)] = globals()['W_conv_cut' + str(j)]\n",
    "                    full_precision_variables['b_conv_cut' + str(j)] = globals()['b_conv_cut' + str(j)]\n",
    "                    \n",
    "                    regularizer += tf.nn.l2_loss(globals()['W_conv_cut' + str(j)])\n",
    "                    \n",
    "                else:\n",
    "                    globals()['h_conv_cut' + str(j)] = globals()['h_conv' + str(i-3)]\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                #--------------block: sum and acitvate\n",
    "                h_add = tf.add(globals()['h_conv_cut' + str(j-1)] , globals()['h_conv' + str(i-1)])\n",
    "                h_add = tf.nn.relu(h_add)\n",
    "        \n",
    "        # Average pooling\n",
    "        h_pool = tf.nn.max_pool(h_add, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "        \n",
    "        # Flaten the h_add output\n",
    "        h_add_flat = tf.reshape(h_pool, shape=(-1, 1*1*128))\n",
    "        \n",
    "        # Dense layer1\n",
    "        W_fc1 = weight_variable(shape=[1*1*128, 1024], name=\"W_fc1\")\n",
    "        b_fc1 = bias_variable(shape=[1024], name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_add_flat, W_fc1) + b_fc1)\n",
    "        full_precision_variables['W_fc1'] = W_fc1\n",
    "        full_precision_variables['b_fc1'] = b_fc1\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_fc1)\n",
    "        \n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        W_fc2 = weight_variable(shape=[1024, 10], name=\"W_fc2\")\n",
    "        b_fc2 = bias_variable(shape=[10], name=\"b_fc2\")\n",
    "        full_precision_variables['W_fc2'] = W_fc2\n",
    "        full_precision_variables['b_fc2'] = b_fc2\n",
    "        \n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        regularizer += tf.nn.l2_loss(W_fc2)\n",
    "        \n",
    "        # Labels\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        y_ = tf.one_hot(y, 10)\n",
    "        \n",
    "        # Defining optimizer and loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "        train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy + reg*regularizer)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Initializer\n",
    "        graph_init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet34\n",
    "#### Save the model and weights for initialization of ABC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 15.5134%  Loss: 7.555779\n",
      "Epoch: 2  Val accuracy: 17.7455%  Loss: 2.224028\n",
      "Epoch: 3  Val accuracy: 19.1964%  Loss: 2.142291\n",
      "Epoch: 4  Val accuracy: 19.9777%  Loss: 2.115969\n",
      "Epoch: 5  Val accuracy: 21.3170%  Loss: 2.050608\n",
      "Epoch: 6  Val accuracy: 25.1116%  Loss: 1.979530\n",
      "Epoch: 7  Val accuracy: 21.4286%  Loss: 1.973415\n",
      "Epoch: 8  Val accuracy: 24.7768%  Loss: 1.940939\n",
      "Epoch: 9  Val accuracy: 26.0045%  Loss: 1.915062\n",
      "Epoch: 10  Val accuracy: 26.7857%  Loss: 1.890285\n",
      "Epoch: 11  Val accuracy: 26.5625%  Loss: 1.874595\n",
      "Epoch: 12  Val accuracy: 28.3482%  Loss: 1.874708\n",
      "Epoch: 13  Val accuracy: 28.3482%  Loss: 1.841272\n",
      "Epoch: 14  Val accuracy: 28.4598%  Loss: 1.810743\n",
      "Epoch: 15  Val accuracy: 31.8080%  Loss: 1.812872\n",
      "Epoch: 16  Val accuracy: 32.3661%  Loss: 1.785321\n",
      "Epoch: 17  Val accuracy: 32.8125%  Loss: 1.751629\n",
      "Epoch: 18  Val accuracy: 31.8080%  Loss: 1.757422\n",
      "Epoch: 19  Val accuracy: 30.0223%  Loss: 1.785910\n",
      "Epoch: 20  Val accuracy: 32.2545%  Loss: 1.737449\n",
      "Epoch: 21  Val accuracy: 34.5982%  Loss: 1.728814\n",
      "Epoch: 22  Val accuracy: 38.6161%  Loss: 1.674386\n",
      "Epoch: 23  Val accuracy: 35.8259%  Loss: 1.692106\n",
      "Epoch: 24  Val accuracy: 37.9464%  Loss: 1.651386\n",
      "Epoch: 25  Val accuracy: 38.2812%  Loss: 1.635164\n",
      "Epoch: 26  Val accuracy: 37.7232%  Loss: 1.686596\n",
      "Epoch: 27  Val accuracy: 38.1696%  Loss: 1.629417\n",
      "Epoch: 28  Val accuracy: 37.9464%  Loss: 1.637519\n",
      "Epoch: 29  Val accuracy: 42.9688%  Loss: 1.556656\n",
      "Epoch: 30  Val accuracy: 44.4196%  Loss: 1.530065\n",
      "Epoch: 31  Val accuracy: 42.1875%  Loss: 1.514104\n",
      "Epoch: 32  Val accuracy: 45.2009%  Loss: 1.486288\n",
      "Epoch: 33  Val accuracy: 47.5446%  Loss: 1.424665\n",
      "Epoch: 34  Val accuracy: 49.1071%  Loss: 1.437206\n",
      "Epoch: 35  Val accuracy: 50.6696%  Loss: 1.360390\n",
      "Epoch: 36  Val accuracy: 50.2232%  Loss: 1.344339\n",
      "Epoch: 37  Val accuracy: 52.6786%  Loss: 1.298116\n",
      "Epoch: 38  Val accuracy: 52.5670%  Loss: 1.304181\n",
      "Epoch: 39  Val accuracy: 56.2500%  Loss: 1.254065\n",
      "Epoch: 40  Val accuracy: 55.6920%  Loss: 1.238380\n",
      "Epoch: 41  Val accuracy: 55.3571%  Loss: 1.234024\n",
      "Epoch: 42  Val accuracy: 55.5804%  Loss: 1.272306\n",
      "Epoch: 43  Val accuracy: 57.0312%  Loss: 1.263353\n",
      "Epoch: 44  Val accuracy: 57.0312%  Loss: 1.245392\n",
      "Epoch: 45  Val accuracy: 59.4866%  Loss: 1.195113\n",
      "Epoch: 46  Val accuracy: 59.1518%  Loss: 1.185621\n",
      "Epoch: 47  Val accuracy: 59.0402%  Loss: 1.169610\n",
      "Epoch: 48  Val accuracy: 61.8304%  Loss: 1.110924\n",
      "Epoch: 49  Val accuracy: 63.2812%  Loss: 1.081372\n",
      "Epoch: 50  Val accuracy: 60.3795%  Loss: 1.106255\n",
      "Epoch: 51  Val accuracy: 61.0491%  Loss: 1.075157\n",
      "Epoch: 52  Val accuracy: 63.6161%  Loss: 1.033623\n",
      "Epoch: 53  Val accuracy: 62.3884%  Loss: 1.017997\n",
      "Epoch: 54  Val accuracy: 62.9464%  Loss: 1.020620\n",
      "Epoch: 55  Val accuracy: 63.7277%  Loss: 0.993074\n",
      "Epoch: 56  Val accuracy: 64.1741%  Loss: 0.974815\n",
      "Epoch: 57  Val accuracy: 64.3973%  Loss: 0.999221\n",
      "Epoch: 58  Val accuracy: 65.0670%  Loss: 0.958969\n",
      "Epoch: 59  Val accuracy: 65.8482%  Loss: 0.950942\n",
      "Epoch: 60  Val accuracy: 65.5134%  Loss: 0.967805\n",
      "Epoch: 61  Val accuracy: 66.8527%  Loss: 0.965601\n",
      "Epoch: 62  Val accuracy: 67.1875%  Loss: 0.933463\n",
      "Epoch: 63  Val accuracy: 68.4152%  Loss: 0.917166\n",
      "Epoch: 64  Val accuracy: 67.8571%  Loss: 0.977945\n",
      "Epoch: 65  Val accuracy: 64.7321%  Loss: 1.087049\n",
      "Epoch: 66  Val accuracy: 69.0848%  Loss: 0.920081\n",
      "Epoch: 67  Val accuracy: 66.4062%  Loss: 0.929443\n",
      "Epoch: 68  Val accuracy: 67.6339%  Loss: 0.914953\n",
      "Epoch: 69  Val accuracy: 67.7455%  Loss: 0.897290\n",
      "Epoch: 70  Val accuracy: 69.8661%  Loss: 0.881644\n",
      "Epoch: 71  Val accuracy: 68.5268%  Loss: 0.901607\n",
      "Epoch: 72  Val accuracy: 70.0893%  Loss: 0.853321\n",
      "Epoch: 73  Val accuracy: 69.3080%  Loss: 0.882793\n",
      "Epoch: 74  Val accuracy: 70.9821%  Loss: 0.826915\n",
      "Epoch: 75  Val accuracy: 67.6339%  Loss: 0.865433\n",
      "Epoch: 76  Val accuracy: 69.6429%  Loss: 0.879564\n",
      "Epoch: 77  Val accuracy: 69.3080%  Loss: 0.846076\n",
      "Epoch: 78  Val accuracy: 70.5357%  Loss: 0.866651\n",
      "Epoch: 79  Val accuracy: 68.7500%  Loss: 0.875714\n",
      "Epoch: 80  Val accuracy: 69.4196%  Loss: 0.852558\n",
      "Epoch: 81  Val accuracy: 69.4196%  Loss: 0.877429\n",
      "Epoch: 82  Val accuracy: 70.8705%  Loss: 0.851661\n",
      "Epoch: 83  Val accuracy: 69.5312%  Loss: 0.864589\n",
      "Epoch: 84  Val accuracy: 70.9821%  Loss: 0.856560\n",
      "Epoch: 85  Val accuracy: 72.2098%  Loss: 0.814976\n",
      "Epoch: 86  Val accuracy: 70.9821%  Loss: 0.791631\n",
      "Epoch: 87  Val accuracy: 71.6518%  Loss: 0.787858\n",
      "Epoch: 88  Val accuracy: 71.7634%  Loss: 0.843824\n",
      "Epoch: 89  Val accuracy: 74.1071%  Loss: 0.768293\n",
      "Epoch: 90  Val accuracy: 70.6473%  Loss: 0.794085\n",
      "Epoch: 91  Val accuracy: 71.3170%  Loss: 0.783919\n",
      "Epoch: 92  Val accuracy: 70.2009%  Loss: 0.824226\n",
      "Epoch: 93  Val accuracy: 72.3214%  Loss: 0.817148\n",
      "Epoch: 94  Val accuracy: 71.6518%  Loss: 0.790596\n",
      "Epoch: 95  Val accuracy: 73.4375%  Loss: 0.775982\n",
      "Epoch: 96  Val accuracy: 71.8750%  Loss: 0.786717\n",
      "Epoch: 97  Val accuracy: 71.8750%  Loss: 0.799441\n",
      "Epoch: 98  Val accuracy: 72.9911%  Loss: 0.762743\n",
      "Epoch: 99  Val accuracy: 71.5402%  Loss: 0.829336\n",
      "Epoch: 100  Val accuracy: 74.4420%  Loss: 0.743226\n",
      "CPU times: user 26min 20s, sys: 4min 8s, total: 30min 29s\n",
      "Wall time: 27min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "num_batch = len(x_train) // batch_size\n",
    "num_batch_val = len(x_val) // batch_size\n",
    "\n",
    "full_precision_values = {}  # values fed to the ABC model\n",
    "cur_model_name = 'ResNet34_full'\n",
    "pre_trained_model = None\n",
    "\n",
    "with tf.Session(graph=full_precision_graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    if pre_trained_model is not None:\n",
    "        try:\n",
    "            print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "            saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Load model Failed!\")\n",
    "    \n",
    "    sess.run(graph_init)\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(1, num_batch + 1):\n",
    "            X_batch, y_batch = x_train[(iteration-1)*batch_size:iteration*batch_size], y_train[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "            _, acc_train, loss_train = sess.run([train_step, accuracy, cross_entropy],\n",
    "                                                feed_dict={x: X_batch, y: y_batch, keep_prob: 0.8})\n",
    "            \n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%) Train Acc: {:.4f}%\".format(iteration,\n",
    "                                                                           num_batch,\n",
    "                                                                           iteration*100/num_batch,\n",
    "                                                                           acc_train*100),\n",
    "                  end=\"\")\n",
    "        \n",
    "        # At the end of each epoch, measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, num_batch_val + 1):\n",
    "            X_batch, y_batch = x_val[(iteration-1)*batch_size:iteration*batch_size], y_val[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            acc_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, num_batch_val,iteration * 100/num_batch_val),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(epoch + 1, acc_val * 100, loss_val))\n",
    "    \n",
    "    # Save model to file\n",
    "    saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "    \n",
    "    # On completion of training, save the variables to be fed to ABC model\n",
    "    for var_name in full_precision_variables:\n",
    "        full_precision_values[var_name] = sess.run(full_precision_variables[var_name])\n",
    "    \n",
    "    # Save weights to file\n",
    "    f = open(\"Weights/\"+cur_model_name+\".pkl\", \"wb\")\n",
    "    pickle.dump(full_precision_values, f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph of ResNet34 with ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ecbm4040/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Creating the ResNet20 graph with ABC\n",
    "num_res_blocks = [3, 4, 6, 3]\n",
    "ABC_graph = tf.Graph()\n",
    "with ABC_graph.as_default():\n",
    "    # Defining inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    \n",
    "    # Store alpha training and alphas\n",
    "    alpha_training_list = []\n",
    "    alphas_list = []\n",
    "    \n",
    "    # Set ABC hyperparameters\n",
    "    M, N = 3, 3\n",
    "    s_min, s_max = -0.2, 0.2\n",
    "    s_gap = (s_max - s_min) / (N-1)\n",
    "    shift_para = tf.Variable(tf.constant(np.arange(s_min, s_max+1e-4, s_gap).tolist(), dtype=tf.float32, name=\"shift_para\"))\n",
    "    betas = tf.Variable(tf.constant(1/N, shape=(N,1)), dtype=tf.float32, name=\"betas\")\n",
    "    \n",
    "    # Create a variable dict to save weights\n",
    "    ABC_variables = {}\n",
    "    \n",
    "    # Process inputs into the required form\n",
    "    num_filters_out = 16\n",
    "    \n",
    "    with tf.variable_scope('resnet_block', reuse=False):\n",
    "        # Record layer index\n",
    "        i = 1\n",
    "        j = 1\n",
    "        \n",
    "        # Convolution Layer 1 before the resnet block\n",
    "        W_conv1 = tf.Variable(full_precision_values[\"W_conv1\"], name=\"W_conv1\")\n",
    "        b_conv1 = tf.Variable(full_precision_values[\"b_conv1\"], name=\"b_conv1\")\n",
    "        # ABC\n",
    "        alphas1 = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name=\"alphas1\")\n",
    "        binary_weights1 = calculate_binary_weights(W_conv1, M)\n",
    "        alpha_loss1, alpha_training1 = calculate_alphas(W_conv1, binary_weights1, alphas1, M)\n",
    "        h_conv1 = ABC_layer(x_image, binary_weights1, alphas1, shift_para, betas, M, N, b_conv1)\n",
    "        # Save alpha training and alphas\n",
    "        alpha_training_list.append(alpha_training1)\n",
    "        alphas_list.append(alphas1)\n",
    "        \n",
    "        bn_conv1 = tf.layers.batch_normalization(h_conv1, axis=-1, name=\"bn_conv1\")\n",
    "        h_conv1 = tf.nn.relu(bn_conv1)\n",
    "        \n",
    "        ABC_variables['W_conv1'] = W_conv1\n",
    "        ABC_variables['b_conv1'] = b_conv1\n",
    "        \n",
    "        # Start resnet block\n",
    "        i += 1\n",
    "        \n",
    "        for stack in range(4):\n",
    "            for res_block in range(num_res_blocks[stack]):\n",
    "                \n",
    "                strides_in = 1\n",
    "                num_filters_in = num_filters_out\n",
    "                num_filters_initial_block = num_filters_in\n",
    "                \n",
    "                #--------------block layer: y path - first y\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = 2  # downsample\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                globals()['W_conv' + str(i)] = tf.Variable(full_precision_values['W_conv' + str(i)], name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = tf.Variable(full_precision_values['b_conv' + str(i)], name='b_conv' + str(i))\n",
    "                # ABC\n",
    "                globals()['alphas' + str(i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(i))\n",
    "                globals()['binary_weights' + str(i)] = calculate_binary_weights(globals()['W_conv' + str(i)], M)\n",
    "                globals()['alpha_loss' + str(i)], globals()['alpha_training' + str(i)] = calculate_alphas(globals()['W_conv' + str(i)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], M)\n",
    "                globals()['conv' + str(i)] = ABC_layer(globals()['h_conv' + str(i-1)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], shift_para, betas, M, N, globals()['b_conv' + str(i)], strides_in)\n",
    "                # save alpha training and alphas\n",
    "                alpha_training_list.append(globals()['alpha_training' + str(i)])\n",
    "                alphas_list.append(globals()['alphas' + str(i)])\n",
    "                \n",
    "                globals()['bn_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                globals()['h_conv' + str(i)] = tf.nn.relu(globals()['bn_conv' + str(i)])\n",
    "                \n",
    "                ABC_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                ABC_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                num_filters_in = num_filters_out\n",
    "                \n",
    "                #--------------block layer: y path - second y\n",
    "                globals()['W_conv' + str(i)] = tf.Variable(full_precision_values['W_conv' + str(i)], name='W_conv' + str(i))\n",
    "                globals()['b_conv' + str(i)] = tf.Variable(full_precision_values['b_conv' + str(i)], name='b_conv' + str(i))\n",
    "                # ABC\n",
    "                globals()['alphas' + str(i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(i))\n",
    "                globals()['binary_weights' + str(i)] = calculate_binary_weights(globals()['W_conv' + str(i)], M)\n",
    "                globals()['alpha_loss' + str(i)], globals()['alpha_training' + str(i)] = calculate_alphas(globals()['W_conv' + str(i)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], M)\n",
    "                globals()['conv' + str(i)] = ABC_layer(globals()['h_conv' + str(i-1)], globals()['binary_weights' + str(i)], globals()['alphas' + str(i)], shift_para, betas, M, N, globals()['b_conv' + str(i)])\n",
    "                # save alpha training and alphas\n",
    "                alpha_training_list.append(globals()['alpha_training' + str(i)])\n",
    "                alphas_list.append(globals()['alphas' + str(i)])\n",
    "                \n",
    "                globals()['h_conv' + str(i)] = tf.layers.batch_normalization(globals()['conv' + str(i)], axis=-1, name='bn_conv' + str(i))\n",
    "                \n",
    "                ABC_variables['W_conv' + str(i)] = globals()['W_conv' + str(i)]\n",
    "                ABC_variables['b_conv' + str(i)] = globals()['b_conv' + str(i)]\n",
    "                \n",
    "                i += 1\n",
    "                \n",
    "                #--------------block layer: x path\n",
    "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                    strides_in = 2  # downsample\n",
    "                    num_filters_in = num_filters_initial_block\n",
    "                    num_filters_out = num_filters_in * 2 \n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    globals()['W_conv_cut' + str(j)] = tf.Variable(full_precision_values['W_conv_cut' + str(j)], name='W_conv_cut' + str(j))\n",
    "                    globals()['b_conv_cut' + str(j)] = tf.Variable(full_precision_values['b_conv_cut' + str(j)], name='b_conv_cut' + str(j))\n",
    "                    # ABC\n",
    "                    globals()['alphas' + str(j*100+i)] = tf.Variable(tf.random_normal(shape=(M,1), mean=1.0, stddev=0.1), dtype=tf.float32, name='alphas' + str(j*100+i))\n",
    "                    globals()['binary_weights' + str(j*100+i)] = calculate_binary_weights(globals()['W_conv_cut' + str(j)], M)\n",
    "                    globals()['alpha_loss' + str(j*100+i)], globals()['alpha_training' + str(j*100+i)] = calculate_alphas(globals()['W_conv_cut' + str(j)], globals()['binary_weights' + str(j*100+i)], globals()['alphas' + str(j*100+i)], M)\n",
    "                    globals()['h_conv_cut' + str(j)] = ABC_layer(globals()['h_conv' + str(i-3)], globals()['binary_weights' + str(j*100+i)], globals()['alphas' + str(j*100+i)], shift_para, betas, M, N, globals()['b_conv_cut' + str(j)], strides_in)\n",
    "                    # save alpha training and alphas\n",
    "                    alpha_training_list.append(globals()['alpha_training' + str(j*100+i)])\n",
    "                    alphas_list.append(globals()['alphas' + str(j*100+i)])\n",
    "                    \n",
    "                    ABC_variables['W_conv_cut' + str(j)] = globals()['W_conv_cut' + str(j)]\n",
    "                    ABC_variables['b_conv_cut' + str(j)] = globals()['b_conv_cut' + str(j)]\n",
    "                    \n",
    "                else:\n",
    "                    globals()['h_conv_cut' + str(j)] = globals()['h_conv' + str(i-3)]\n",
    "                \n",
    "                j += 1\n",
    "                \n",
    "                #--------------block: sum and acitvate\n",
    "                h_add = tf.add(globals()['h_conv_cut' + str(j-1)] , globals()['h_conv' + str(i-1)])\n",
    "                h_add = tf.nn.relu(h_add)\n",
    "        \n",
    "        # Average pooling\n",
    "        h_pool = tf.nn.max_pool(h_add, ksize=[1,4,4,1], strides=[1,4,4,1], padding=\"SAME\")\n",
    "        \n",
    "        # Flaten the h_add output\n",
    "        h_add_flat = tf.reshape(h_pool, shape=(-1, 1*1*128))\n",
    "        \n",
    "        # Dense layer1\n",
    "        W_fc1 = tf.Variable(full_precision_values[\"W_fc1\"], name=\"W_fc1\")\n",
    "        b_fc1 = tf.Variable(full_precision_values[\"b_fc1\"], name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_add_flat, W_fc1) + b_fc1)\n",
    "        ABC_variables['W_fc1'] = W_fc1\n",
    "        ABC_variables['b_fc1'] = b_fc1\n",
    "        \n",
    "        # Dropout\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        \n",
    "        # Output layer\n",
    "        W_fc2 = tf.Variable(full_precision_values[\"W_fc2\"], name=\"W_fc2\")\n",
    "        b_fc2 = tf.Variable(full_precision_values[\"b_fc2\"], name=\"b_fc2\")\n",
    "        ABC_variables['W_fc2'] = W_fc2\n",
    "        ABC_variables['b_fc2'] = b_fc2\n",
    "        \n",
    "        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        # Labels\n",
    "        y = tf.placeholder(tf.int32, [None])\n",
    "        y_ = tf.one_hot(y, 10)\n",
    "        \n",
    "        # Defining optimizer and loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Initializer\n",
    "        graph_init = tf.global_variables_initializer()\n",
    "        alphas_init = tf.variables_initializer(alphas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet34 with ABC\n",
    "#### Save the model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 12.9464%  Loss: 2.312686\n",
      "Epoch: 2  Val accuracy: 13.1696%  Loss: 2.240811\n",
      "Epoch: 3  Val accuracy: 9.4866%  Loss: 2.301382\n",
      "Epoch: 4  Val accuracy: 8.8170%  Loss: 2.324825\n",
      "Epoch: 5  Val accuracy: 12.7232%  Loss: 2.239714\n",
      "Epoch: 6  Val accuracy: 15.9598%  Loss: 2.236971\n",
      "Epoch: 7  Val accuracy: 13.1696%  Loss: 2.265183\n",
      "Epoch: 8  Val accuracy: 10.0446%  Loss: 2.266706\n",
      "Epoch: 9  Val accuracy: 12.3884%  Loss: 2.252622\n",
      "Epoch: 10  Val accuracy: 12.0536%  Loss: 2.264683\n",
      "Epoch: 11  Val accuracy: 12.2768%  Loss: 2.265477\n",
      "Epoch: 12  Val accuracy: 13.2812%  Loss: 2.250084\n",
      "Epoch: 13  Val accuracy: 14.0625%  Loss: 2.239535\n",
      "Epoch: 14  Val accuracy: 10.4911%  Loss: 2.258742\n",
      "Epoch: 15  Val accuracy: 13.0580%  Loss: 2.256282\n",
      "Epoch: 16  Val accuracy: 10.0446%  Loss: 2.261596\n",
      "Epoch: 17  Val accuracy: 10.1562%  Loss: 2.258757\n",
      "Epoch: 18  Val accuracy: 12.6116%  Loss: 2.277562\n",
      "Iteration: 247/382 (64.7%) Train Acc: 13.2812%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "num_batch = len(x_train) // batch_size\n",
    "num_batch_val = len(x_val) // batch_size\n",
    "num_alpha_epochs = 200\n",
    "\n",
    "ABC_values = {}  # values fed to the ABC model\n",
    "cur_model_name = 'ResNet34_ABC_3_3'\n",
    "pre_trained_model = None\n",
    "\n",
    "with tf.Session(graph=ABC_graph) as sess:\n",
    "    writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    if pre_trained_model is not None:\n",
    "        try:\n",
    "            print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "            saver.restore(sess, 'model/{}'.format(pre_trained_model))\n",
    "        except Exception:\n",
    "            raise ValueError(\"Load model Failed!\")\n",
    "    \n",
    "    sess.run(graph_init)\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(1, num_batch + 1):\n",
    "            X_batch, y_batch = x_train[(iteration-1)*batch_size:iteration*batch_size], y_train[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            \n",
    "            # Training alphas\n",
    "            sess.run(alphas_init)\n",
    "            for alpha_train in alpha_training_list:\n",
    "                for alpha_epoch in range(num_alpha_epochs):\n",
    "                    sess.run(alpha_train)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "            _, acc_train, loss_train = sess.run([train_step, accuracy, cross_entropy],\n",
    "                                                feed_dict={x: X_batch, y: y_batch, keep_prob: 0.8})\n",
    "            \n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%) Train Acc: {:.4f}%\".format(iteration,\n",
    "                                                                           num_batch,\n",
    "                                                                           iteration*100/num_batch,\n",
    "                                                                           acc_train*100),\n",
    "                  end=\"\")\n",
    "        \n",
    "        # At the end of each epoch, measure the validation loss and accuracy:\n",
    "        # Training alphas\n",
    "        sess.run(alphas_init)\n",
    "        for alpha_train in alpha_training_list:\n",
    "            for alpha_epoch in range(num_alpha_epochs):\n",
    "                sess.run(alpha_train)\n",
    "        \n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, num_batch_val + 1):\n",
    "            X_batch, y_batch = x_val[(iteration-1)*batch_size:iteration*batch_size], y_val[(iteration-1)*batch_size:iteration*batch_size]\n",
    "            y_batch = y_batch.reshape(batch_size)\n",
    "            acc_val, loss_val = sess.run([accuracy, cross_entropy], feed_dict={x: X_batch, y: y_batch, keep_prob: 1})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, num_batch_val,iteration * 100/num_batch_val),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(epoch + 1, acc_val * 100, loss_val))\n",
    "    \n",
    "    # Save model to file\n",
    "    saver.save(sess, 'model/{}'.format(cur_model_name))\n",
    "    \n",
    "    # On completion of training, save the variables to be fed to ABC model\n",
    "    for var_name in ABC_variables:\n",
    "        ABC_values[var_name] = sess.run(ABC_variables[var_name])\n",
    "    \n",
    "    # Save weights to file\n",
    "    f = open(\"Weights/\"+cur_model_name+\".pkl\", \"wb\")\n",
    "    pickle.dump(ABC_values, f)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
